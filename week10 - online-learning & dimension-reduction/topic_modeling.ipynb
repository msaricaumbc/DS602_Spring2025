{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7541b37c-4342-4113-883d-304fed567123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2676380d-ec10-4ffd-a656-1952d44c9d40",
   "metadata": {},
   "source": [
    "# Brief interlude for topic modeling\n",
    "50,000 reviews is way too many documents for individuals to read in order to get an impression of what people are saying. We can use `Latent Dirichlet Allocation` to reduce the text into topic vectors so we understand what is being stated (generally) in the documents with relatively little overhead.\n",
    "\n",
    "In a nutshell, it finds words that frequently appear together across documents. It uses two matrices (1) `document-to-topic` and (2) `word-to-topic` that when multiplied together reproduce the bag-of-words matrix.  Like PCA, you `need to define the number of topics`.\n",
    "\n",
    "Starting with the word frequencies of the `imdb` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66a7d90-7e8b-40e6-9d17-ae5d9d6ad689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "# exclude words that occur in more than 10% of the documents\n",
    "# include most common 5000 words\n",
    "count = CountVectorizer(stop_words='english', max_df=.1, max_features=5000)\n",
    "\n",
    "X = count.fit_transform(df['review'].values)\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, \n",
    "                               random_state=10, \n",
    "                               learning_method='batch'\n",
    "                              )\n",
    "\n",
    "X_topics = lda.fit_transform(X)\n",
    "\n",
    "en = datetime.datetime.now()\n",
    "\n",
    "el = en-st\n",
    "\n",
    "print(f'Time to complete: {el}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5737f19d-a316-40ac-9704-7ac29bc155aa",
   "metadata": {},
   "source": [
    "Word importances for each of the 5,000 words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c32d5-fe57-4610-8d3f-379a9759b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642c92f-9d44-4530-8fa1-00abe0d2c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5\n",
    "\n",
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Topic {topic_idx+1}')\n",
    "    print(','.join([feature_names[i] for i in topic.argsort()[:-top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaef4ca-5482-4ddb-9150-0b4c96426431",
   "metadata": {},
   "source": [
    "> You would need to convert these into logical categories to present to your boss. You would want to check the topics match the text, since LDA can extract topics that aren't much stronger than random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dded61-803d-4ba8-9f2c-08497572b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy = X_topics[:, 0].argsort()[::-1]\n",
    "for iter_idx, movie_idx in enumerate(comedy[:3]):\n",
    "    print(f'\\nComedy {iter_idx+1}:')\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046b66a-10b9-4957-92ad-46adf2691c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Topic {topic_idx+1}')\n",
    "    print(','.join([feature_names[i] for i in topic.argsort()[:-top_words - 1:-1]]))\n",
    "    \n",
    "    topic = X_topics[:, topic_idx].argsort()[::-1]\n",
    "    \n",
    "    for iter_idx, movie_idx in enumerate(topic[:3]):\n",
    "        print(f'- {iter_idx+1}:')\n",
    "        print(df['review'][movie_idx][:300], '...')\n",
    "        \n",
    "    print ('------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50b35f-6415-4e33-84c3-d29bf92c96ad",
   "metadata": {},
   "source": [
    "> Seem reasonable?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "\n",
    "## Agenda\n",
    "- Why do we consider dimensionality reduction techniques? \n",
    "- PCA. \n",
    "- Kernel PCA.  \n",
    "- LDA.  \n",
    "- Manifold with t-SNE.   \n",
    "\n",
    "\n",
    "Generally following `Machine Learning with Python 3rd Edition, Chapter 5`.\n",
    "\n",
    "- Also known as feature extraction.  \n",
    "- Maps the original feature space into a lower subspace with lower dimensionality than the original one.  \n",
    "- This lower dimensional subspace should be less sparse, have features that are less correlated with each other, and explain a significant amount of the original variance in the full feature space.\n",
    "\n",
    "### Potential Benefits\n",
    "- Less data to deal with.  \n",
    "- Less spareness.  \n",
    "- Helps with overfitting and bias issues.  \n",
    "- Can leverage compression to be able to [visualize high-dimensional data](https://projector.tensorflow.org) in lower dimensions that retain characteristics of the overall variance.  \n",
    "- Can make the data more conducive to modeling with kernel transformations.  \n",
    "- May be able to visualize the high-dimensional data in 2 or 3-dimension space.\n",
    "\n",
    "### Cons  \n",
    "- Not as intrepretable.  \n",
    "- Some compression methods can be computationally intensive.  \n",
    "- This simplifies the features, so there is some amount of information loss that simple models may need, but other, more complex models may be better using the raw data.\n",
    "\n",
    "> Think of this as compression, essentially the same thing as compressing a large file into a .zip file. The file is still there, but it has been managed to be compressed to a smaller file size.\n",
    "\n",
    "This can also help with the `curse of dimensionality`. Since we will be compressing the space and reducing the spareness, it should squeeze the data closer together in the new subspace.\n",
    "\n",
    "The `curse of dimensionality` can make training slow since we have may have a large, sparse space and it can make finding solutions difficult.  \n",
    "- In a 1 x 1 box, 0.4% of random points would be within 0.001 from one of the borders, i.e., extreme observations along one of the dimensions.  \n",
    "- In a 10,000 x 10,000 space, nearly 100% of the observations would be near a border, i.e., across enough dimensions, almost every point will be extreme if you considered all the dimensions.\n",
    "\n",
    "- If you pick two point at random on a unit square, the distance will be on average 0.52 units away.  \n",
    "- If you pick two point at random on a 3-D cube, the distance will be on average 0.66 units away.  \n",
    "- In a 1,000,000-dimension hypercube, the average distance is about 408 units.\n",
    "\n",
    "> Since points will be so much further away in higher dimensions, the predictions will be less reliable than in lower dimensions.\n",
    "\n",
    "### Visualization example: 3-dimensional data lying near a 2-dimensional plane\n",
    "Think of a dataset with 3 continuous features, and one of the features being essentially worthless in terms of its value to separating the data.\n",
    "\n",
    "<img src='./diagrams/3d-data-close-to-2d.png'>\n",
    "\n",
    "[Image source: Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow](https://github.com/ageron/handson-ml2)\n",
    "\n",
    "### After projection to a 2-dimensional space\n",
    "\n",
    "<img src='./diagrams/2d-data-post-projection.png'>\n",
    "\n",
    "[Image source: Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow](https://github.com/ageron/handson-ml2)\n",
    "\n",
    "# Not everything works with simple projections\n",
    "\n",
    "### Swiss Roll\n",
    "[See scikit-learn's datasets if you want to examine the data.](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_swiss_roll.html)\n",
    "\n",
    "<img src='./diagrams/swiss-roll.png'>\n",
    "\n",
    "[Image source: Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow](https://github.com/ageron/handson-ml2)\n",
    "\n",
    "#### Projecting versus unrolling\n",
    "Left shows a projection similar to the 3-dimensional to 2-dimensional example. The right shows a different technique that looks at the manifold and tries to learn the shape.\n",
    "\n",
    "<img src='./diagrams/swiss-unroll.png'>\n",
    "\n",
    "[Image source: Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow](https://github.com/ageron/handson-ml2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components (PCA)\n",
    "PCA is a projection technique. In many cases data is not spread uniformly across all dimensions, it'll be concentrated among a subset like the 3-dimensional to 2-dimensional example above. We can project the dataset to a lower dimensional subspace without losing much of the information. Projections won't work well for data like the `swiss roll` example since the data may twist and turn and will require a [manifold](https://en.wikipedia.org/wiki/Manifold) technique.\n",
    "\n",
    "- Is an unsupervised learning modeling.   \n",
    "- Very common method and has been around since 1901.  \n",
    "- Helps with dimensionality reduction and remove of noise in the data.  \n",
    "- Commonly used as a preprocessing step. \n",
    "- Shouldn't be used blindly or purely to regularize your data, i.e., you should have just cause for using it.  \n",
    "\n",
    "<img src='./diagrams/pca.png' style='width: 600px'>\n",
    "\n",
    "### Basic Idea\n",
    "- Finds directions of maximum variance in high-dimension space and projects it to a lower space with the same or fewer dimensions.  \n",
    "- The components are orthogonal axes of the subspace and are the directions of maximum variance.\n",
    "- We construct a $d \\times k-dimensional$ transformation matrix, $W$, that maps the vector, $x$, the features of the training data to the new subspace. The new subspace will have fewer dimensions then the original $d$ dimensions.  \n",
    "- It starts with looking at the hyperplane that lies closest to the data, and projects the data to it.\n",
    "\n",
    "$$\n",
    "x=[x_1, x_2, \\dots, x_d], x \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "That is transformed by a matrix, $W \\in \\mathbb{R}^{d \\times k}$:\n",
    "\n",
    "$$\n",
    "xW=z, \\space \\text{where } z = [z_1, z_2, \\dots, z_k], z \\in \\mathbb{R}^k\n",
    "$$\n",
    "\n",
    "$d$-dimensional data is now projected to $k$-dimesions, generally $k<<d$.\n",
    "- The first principal component will explain the largest amount of variance, the second the second most variance, ...  \n",
    "- Very sensitive to scaling, so you'll need to standardize the data prior. If you don't the features will have different relative importances, which we probably don't want.  \n",
    "- As noted earlier, the components are uncorrelated, even if the features are correlated. This can be handy for dealing with multicollinearity issues.\n",
    "\n",
    "> Minor issue: small differences in the training set, e.g., you find more examples or update a feature, could cause different results when you run PCA.\n",
    "\n",
    "### Steps in the Process\n",
    "- Standardize the feature matrix (`min-max` or `standardize`).  \n",
    "- Create covariance matrix.  \n",
    "- Decompose into eigenvectors and eigenvalues.  \n",
    "- Sort eigenvalues by decreasing order to rank the eigenvectors.  \n",
    "- Select $k$ eigenvectors. This is the primary tuning knob.  \n",
    "- Construct the projection matrix.  \n",
    "- Transform the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wine = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/ensembles/wine.data', header=None)\n",
    "wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.plotting.scatter_matrix(wine.iloc[:,1:], figsize=(9,9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Many features that at least visually seem to be highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Split and Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = wine.iloc[:, 1:].values, wine.iloc[:, 0].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=0\n",
    "                                                   )\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std  = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Covariance Matrix\n",
    "Pairwise covariances between features $j$ and $k$.\n",
    "\n",
    "$$\n",
    "\\sigma_{jk} = \\frac{1}{n-1}\\sum{_{i=1}^n}(x_j^{(i)}-\\mu_k)\n",
    "$$\n",
    "\n",
    "Since the data is standardized, $\\mu_k = 0$, so a positive covariance means the features increase together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cov_mat = np.cov(X_train_std.T)\n",
    "print(cov_mat.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Eigenpairs of Covariance Matrix\n",
    "Extracted the 13 eigenvalues, same as $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "plt.plot(eigen_vals)\n",
    "plt.title('Eigenvalues', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vecs\n",
    "eigen_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Sort & Step 5: Selecting the meaning $k$ Eigenvalues\n",
    "- Eigenvalues define the magnitude of the eigenvector, so they need to be sorted.  \n",
    "- Want to select the top $k$, usually based on how much variance they explain.\n",
    "\n",
    "$$\n",
    "\\text{Explained variance ratio} = \\frac{\\lambda_j}{\\sum \\lambda_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eigen_vals)\n",
    "\n",
    "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "plt.bar(range(1,14), var_exp, alpha=0.5, align='center', label='Variance Explained')\n",
    "plt.step(range(1,14), cum_var_exp, where='mid', label='Cumulative Explained')\n",
    "plt.xlabel('Principal Componment')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First component explains nearly 40% of the variance in the data.  \n",
    "- First and second explain almost 60%. So we'll set $k=2$.  \n",
    "- Altogether they will sum to 100%.\n",
    "\n",
    "- Note: PCA is an unsupervised method which means that information about the class is ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Projection Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]\n",
    "eigen_pairs.sort(key=lambda k: k[0], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract first $2$ eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "              eigen_pairs[1][1][:, np.newaxis]))\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Transform\n",
    "$$\n",
    "X'=XW\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = X_train_std.dot(w)\n",
    "X_train_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train_pca[:,0], X_train_pca[:,1], 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction\n",
    "\n",
    "$$\n",
    "X_{recovered}=X_{d-projection}W_{d}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using within a Model\n",
    "- $k$ can be treated as a hyperparameter.  \n",
    "- Use GridSearch to determine the best value for $k$ - usually searching around 90-95% of explained variance is the best starting point.  \n",
    "- You could also look at the explained variance plot and look for an elbow, but that isn't going to be as quantitatively based and may not maximize predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.iloc[:, 0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.iloc[:, 1:],\n",
    "                                                   wine.iloc[:, 0],\n",
    "                                                   test_size=0.20\n",
    "                                                   )\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "p = Pipeline([('scaling', StandardScaler()), \n",
    "              ('pca', PCA()),\n",
    "              ('model', LogisticRegression())\n",
    "             ])\n",
    "\n",
    "params = {'model__C': [0.01, 0.1, 1, 10], 'pca__n_components': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "\n",
    "wine_search = GridSearchCV(p, param_grid=params, scoring='accuracy', cv=10, refit=True)\n",
    "wine_search = wine_search.fit(X_train, y_train)\n",
    "\n",
    "wine_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Validation score: {wine_search.best_score_:.2%}')\n",
    "print(f'Test score: {wine_search.score(X_test, y_test):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using with Visualization\n",
    "If using to visualize the data, it will only make sense to look at 2 or 3 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = wine.iloc[:, 1:], wine.iloc[:, 0]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "p = Pipeline([('scaling', StandardScaler()), \n",
    "              ('pca', PCA(n_components=3))\n",
    "             ])\n",
    "\n",
    "new_X = p.fit_transform(X)\n",
    "new_X = pd.DataFrame(new_X)\n",
    "new_wine = pd.concat([new_X, pd.Series(y)], axis=1)\n",
    "new_wine.columns = ['comp1', 'comp2', 'comp3', 'class']\n",
    "new_wine['class'] = new_wine['class'].astype('str')\n",
    "new_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x='comp1', y='comp2', hue='class', data=new_wine)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Can help visualize if the 1st and 2nd (and/or 3rd) have a high cumulative explained variance ratio, otherwise, may not help if you have trying to visualize distance between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistX = mnist['data']\n",
    "mnistX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistY = mnist['target']\n",
    "mnistY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample to make this run faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 2500\n",
    "\n",
    "mnist_sample = mnist.data.sample(2500)\n",
    "mnist_sample_targets = mnist.target.iloc[mnist_sample.index.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See if PCA is beneficial for reducing the space\n",
    "Original space is 784 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will take a couple minutes to run.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist_sample,\n",
    "                                                   mnist_sample_targets,\n",
    "                                                   test_size=0.20\n",
    "                                                   )\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "p = Pipeline([('scaling', StandardScaler()), \n",
    "              ('pca', PCA()),\n",
    "              ('model', LogisticRegression(solver='liblinear'))\n",
    "             ])\n",
    "\n",
    "params = {'model__C': [0.01, 10, 100], 'pca__n_components': [25, 50, 100]}\n",
    "\n",
    "mnist_search = GridSearchCV(p, param_grid=params, scoring='accuracy', cv=10, refit=True)\n",
    "mnist_search = mnist_search.fit(X_train, y_train)\n",
    "\n",
    "mnist_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Validation score: {mnist_search.best_score_:.2%}')\n",
    "print(f'Test score: {mnist_search.score(X_test, y_test):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Perform a secondary search to find \"better\" hyperparameter values. We won't do this with every example, but recall this is best practice for thoroughness. We could scan every possible combinations initially, but that will take a long time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'model__C': [5, 10, 15], 'pca__n_components': [40, 50, 60]}\n",
    "\n",
    "mnist_search = GridSearchCV(p, param_grid=params, scoring='accuracy', cv=10, refit=True)\n",
    "mnist_search = mnist_search.fit(X_train, y_train)\n",
    "\n",
    "mnist_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Validation score: {mnist_search.best_score_:.2%}')\n",
    "print(f'Test score: {mnist_search.score(X_test, y_test):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "X_recovered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(instances, images_per_row=5, **options):\n",
    "    '''\n",
    "    From: https://github.com/ageron/handson-ml2/blob/master/08_dimensionality_reduction.ipynb\n",
    "    '''\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "\n",
    "    # This is equivalent to n_rows = ceil(len(instances) / images_per_row):\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "\n",
    "    # Append empty images to fill the end of the grid, if needed:\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    padded_instances = np.concatenate([instances, np.zeros((n_empty, size * size))], axis=0)\n",
    "\n",
    "    # Reshape the array so it's organized as a grid containing 28×28 images:\n",
    "    image_grid = padded_instances.reshape((n_rows, images_per_row, size, size))\n",
    "\n",
    "    # Combine axes 0 and 2 (vertical image grid axis, and vertical image axis),\n",
    "    # and axes 1 and 3 (horizontal axes). We first need to move the axes that we\n",
    "    # want to combine next to each other, using transpose(), and only then we\n",
    "    # can reshape:\n",
    "    big_image = image_grid.transpose(0, 2, 1, 3).reshape(n_rows * size,\n",
    "                                                         images_per_row * size)\n",
    "    # Now that we have a big image, we just need to show it:\n",
    "    plt.imshow(big_image, **options)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::20])\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered[::20])\n",
    "plt.title(\"Compressed\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up PCA\n",
    "- PCA can be useful reducing the dimensions without significantly loss of information. \n",
    "- Only do when necessary (try regularization first).  \n",
    "- The number of components needs to be treated as a hyperparameter.  \n",
    "- If there are a lot of features, searching around 90-99% of explained variance ratio usually ideal.  \n",
    "- PCA uses singular value decomposition (SVD), but scikit-learn has solvers that approximate SVD that are dramatically faster. It generally uses this `randomized` solver, but it may use the `full` SVD.  \n",
    "- There is a variant, `Incremental PCA` that allows for only part of the dataset to be loaded at once. We'll talk about this in a couple weeks, but it is an option for larger datasets.  \n",
    "- For sparse datasets, see [SparsePCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA\n",
    "Maps samples into a higher dimensional space, which allows for non-linear transformations, which we've seen previously with kernels in support vector machines.\n",
    "\n",
    "<img src='./diagrams/sklearn-kernel-pca.png' style='width: 600px'>\n",
    "\n",
    "[Image source: scikit-learn](https://scikit-learn.org/stable/modules/decomposition.html#kernel-pca)\n",
    "\n",
    "\n",
    "See [Scholkopf's notes on *Kernel Principal Component Analysis*](http://pca.narod.ru/scholkopf_kernel.pdf)\n",
    "\n",
    "- While PCA and KernelPCA are unsupervised, KernelPCA doesn't have some of the features as PCA.  \n",
    "- No explained variance methods, so we have to rely on predictive performance and hyperparameter tuning.  \n",
    "- Best to use this as a step within a pipeline and try different kernels and components extracted during grid search.  \n",
    "- Another option is to look at the reconstruction / compression error and pick the hyperparameters that offer the lowest reconstruction error. This would be similar to the above example using the MNIST data.  \n",
    "\n",
    "> Recall that kernel tricks will be a bit more computationally intensive and may not increase performance. Unless the data is nonlinear and would benefit from higher order transformations, it may be of little use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "p = Pipeline([('scaling', StandardScaler()), \n",
    "              ('kpca', KernelPCA()),\n",
    "              ('model', LogisticRegression(solver='liblinear'))\n",
    "             ])\n",
    "\n",
    "params = {'model__C': [0.01],\n",
    "          'kpca__n_components': [10, 100, 150],\n",
    "          'kpca__kernel': ['poly', 'rbf']\n",
    "         }\n",
    "\n",
    "mnist_search = GridSearchCV(p, param_grid=params, scoring='accuracy', cv=10, refit=True)\n",
    "mnist_search = mnist_search.fit(X_train, y_train)\n",
    "\n",
    "mnist_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Validation score: {mnist_search.best_score_:.2%}')\n",
    "print(f'Test score: {mnist_search.score(X_test, y_test):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Not much improvement in the MNIST data. We would see a pronounced difference if the data was similar to the first figure in this section, i.e., highly non-linear.\n",
    "\n",
    "> Remember, we'll want to do at least a secondary pass on the hyperparameters for complete due diligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_circles(n_samples=1_000, factor=0.3, noise=0.05, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, (train_ax, test_ax) = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "\n",
    "train_ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
    "train_ax.set_ylabel(\"Feature #1\")\n",
    "train_ax.set_xlabel(\"Feature #0\")\n",
    "train_ax.set_title(\"Training data\")\n",
    "\n",
    "test_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\n",
    "test_ax.set_xlabel(\"Feature #0\")\n",
    "_ = test_ax.set_title(\"Testing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "kernel_pca = KernelPCA(\n",
    "    n_components=2, \n",
    "    kernel=\"rbf\", \n",
    "    gamma=10, \n",
    ")\n",
    "\n",
    "X_test_pca = pca.fit(X_train).transform(X_test)\n",
    "X_test_kernel_pca = kernel_pca.fit(X_train).transform(X_test)\n",
    "fig, (orig_data_ax, pca_proj_ax, kernel_pca_proj_ax) = plt.subplots(\n",
    "    ncols=3, figsize=(14, 4)\n",
    ")\n",
    "\n",
    "orig_data_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\n",
    "orig_data_ax.set_ylabel(\"Feature #1\")\n",
    "orig_data_ax.set_xlabel(\"Feature #0\")\n",
    "orig_data_ax.set_title(\"Testing data\")\n",
    "\n",
    "pca_proj_ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test)\n",
    "pca_proj_ax.set_ylabel(\"Principal component #1\")\n",
    "pca_proj_ax.set_xlabel(\"Principal component #0\")\n",
    "pca_proj_ax.set_title(\"Projection of testing data\\n using PCA\")\n",
    "\n",
    "kernel_pca_proj_ax.scatter(X_test_kernel_pca[:, 0], X_test_kernel_pca[:, 1], c=y_test)\n",
    "kernel_pca_proj_ax.set_ylabel(\"Principal component #1\")\n",
    "kernel_pca_proj_ax.set_xlabel(\"Principal component #0\")\n",
    "_ = kernel_pca_proj_ax.set_title(\"Projection of testing data\\n using KernelPCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pipeline([('scaling', StandardScaler()), \n",
    "              ('kpca', KernelPCA()),\n",
    "              ('model', LogisticRegression(solver='liblinear'))\n",
    "             ])\n",
    "\n",
    "params = {\n",
    "        'model__C': [0.01],\n",
    "        'kpca__gamma': [None, 2, 10, 50],\n",
    "        'kpca__kernel': [ 'rbf']\n",
    "         }\n",
    "\n",
    "circle_search = GridSearchCV(p, param_grid=params, scoring='accuracy', cv=10, refit=True)\n",
    "circle_search = circle_search.fit(X_train, y_train)\n",
    "\n",
    "circle_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Validation score: {circle_search.best_score_:.2%}')\n",
    "print(f'Test score: {circle_search.score(X_test, y_test):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA)\n",
    "- Similar in concept to PCA and will have many of the same methods, e.g., `explained_variance_ratio_`.  \n",
    "- Actually a supervised learning algorithm.  \n",
    "- Looks at finding axes (hyperplanes) that discriminant the most between classes.  \n",
    "- Unlike PCA, this keeps the classes as far apart as possible, which isn't a trait of PCA.  \n",
    "- Though you'd assume this may work better than PCA, generally PCA is a better option.\n",
    "\n",
    "<img src='./diagrams/lda5-6.png' style='width: 600px'>\n",
    "\n",
    "[Image source Machine Learning with Python 3rd Edition, page 160](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch05/images)\n",
    "\n",
    "- LDA assumes the data is normally distributed.  \n",
    "- In the above, `LD 1` would separate the classes well.  \n",
    "- `LD 2`wouldn't be a good discriminant since it wouldn't separate the data if you were able to draw a horizontal line on the y-axis.  \n",
    "- Similar to linear regression, it assumes independence between the samples, which is really a soft requirement, as it can still work reasonably well if that is violated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Steps with LDA\n",
    "- Standardize the $d$-dimensional dataset.  \n",
    "- Compute the $d$-dimensional mean vector.  \n",
    "- Create between-class scatter matrix, $S_B$ and within-class scatter matrix, $S_w$.  \n",
    "- Calculate [eigenvectors](https://mathworld.wolfram.com/Eigenvector.html) and [eigenvalues](https://mathworld.wolfram.com/Eigenvalue.html) for $S_{W}^{-1}S_B$.  \n",
    "- Sort the eigenvalues to rank eigenvectors.  \n",
    "- Pick $k$ eigenvectors and create a $d\\times k$-dimensional matrix, $W$.  \n",
    "- Project examples using $W$.\n",
    "\n",
    "> If there is perfect collinearity, only one component will be returned. This should be very rare and more a byproduct of a potential error in your analysis.\n",
    "\n",
    "Similar steps to PCA. The class label comes into play with the mean vector.\n",
    "\n",
    "See [Machine Learning with Python 3rd Edition, Pages 161-164 for manual implementation](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.iloc[:, 1:],\n",
    "                                                   wine.iloc[:, 0],\n",
    "                                                   test_size = 0.2\n",
    "                                                   )\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train_scale = ss.fit_transform(X_train)\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "x_train_lda = lda.fit_transform(x_train_scale, y_train)\n",
    "\n",
    "x_train_lda[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Outputs components and their explained variance ratios, similar to PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.score(x_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_scale = ss.transform(X_test)\n",
    "\n",
    "lda.score(x_test_scale, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And the same prediction methods from other classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.iloc[:, 1:],\n",
    "                                                   wine.iloc[:, 0],\n",
    "                                                   test_size=0.20\n",
    "                                                   )\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "p = Pipeline([('scaling', StandardScaler()), \n",
    "              ('lda', LDA()),\n",
    "              ('model', LogisticRegression(solver='liblinear'))\n",
    "             ])\n",
    "\n",
    "params = {'model__C': [0.001, 0.1, 1, 10], 'lda__n_components': [1, 2]}\n",
    "\n",
    "wine_search = GridSearchCV(p, param_grid=params, scoring='accuracy', cv=10, refit=True)\n",
    "wine_search = wine_search.fit(X_train, y_train)\n",
    "\n",
    "wine_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Validation score: {wine_search.best_score_:.2%}')\n",
    "print(f'Test score: {wine_search.score(X_test, y_test):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Near perfect accuracy on the `wine` data, though PCA had similar performance, albeit with more components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Manifold Learning](https://scikit-learn.org/stable/modules/manifold.html)\n",
    "Approach for nonlinear dimension reduction, but mainly used for visualizing data.  \n",
    "\n",
    "<img src='./diagrams/manifold.png'>\n",
    "\n",
    "[Image source scikit-learn manifold](https://scikit-learn.org/stable/modules/manifold.html#manifold)\n",
    "\n",
    "- Looks for how examples relate linearly to their closest neighbors.  \n",
    "- Looks for lower dimensions that preserve those relations.  \n",
    "- Very common for visualizing higher dimension data.\n",
    "- Here's a [short video](https://www.youtube.com/watch?v=wvsE8jm1GzE) that describes the concept at a high-level.  \n",
    "- Most common use-case for these is visualization. See [the embedding projector](https://projector.tensorflow.org/) for an interesting example.\n",
    "\n",
    "## t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "From [sci-kit learn](https://scikit-learn.org/stable/modules/manifold.html):\n",
    ">t-SNE (TSNE) converts affinities of data points to probabilities. The affinities in the original space are represented by Gaussian joint probabilities and the affinities in the embedded space are represented by Student’s t-distributions. This allows t-SNE to be particularly sensitive to local structure and has a few other advantages over existing techniques:\n",
    "\n",
    "> - Better at revealing the structure at many scales on a single map  \n",
    "> - Better at revealing data that lie in multiple, different, manifolds or clusters  \n",
    "> - Better at reducing the tendency to crowd points together at the center  \n",
    "> - While Isomap, LLE and variants are best suited to unfold a single continuous low dimensional manifold, t-SNE will focus on the local structure of the data and will tend to extract clustered local groups of samples as highlighted on the S-curve example. This ability to group samples based on the local structure might be beneficial to visually disentangle a dataset that comprises several manifolds at once as is the case in the digits dataset.\n",
    "\n",
    "> The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence.\n",
    "\n",
    "The disadvantages to using t-SNE are roughly:\n",
    "\n",
    "- t-SNE is computationally expensive, and can take several hours on million-sample datasets where PCA will finish in seconds or minutes  \n",
    "- The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.  \n",
    "- The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error.  \n",
    "- Global structure is not explicitly preserved. This problem is mitigated by initializing points with PCA (using init='pca').\n",
    "\n",
    "[See *How to Use t-SNE Effectively* for best practices and tuning tips.](https://distill.pub/2016/misread-tsne/)\n",
    "\n",
    "### Math Behind SNE (Stochastic Neighbor Embedding)\n",
    "[See Maaten's paper overviewing the technique](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)\n",
    "\n",
    "- Convert the Euclidean distances into conditional probabilities that represent similarities. Similarity of $x_j$ to $x_i$ is the conditional probability that $x_i$ would pick $x_j$ as one of its neighbors. Neighbors are picked according to their probability density, centered at $x_i$.\n",
    "- $p_{j|i}$ will be high for nearby points and approach $0$ for faraway points.  \n",
    "\n",
    "$$\n",
    "p_{j|i}=\\frac{exp(-||x_i-x_j||^2/2\\sigma_i^2)}{\\sum_{k\\neq i}exp(-||x_i-x_k||^2/2\\sigma_i^2)}\n",
    "$$\n",
    "\n",
    "Where $\\sigma_i$ is the variance of the Gaussian centered at x_i. \n",
    "\n",
    "- $x_i$ and $x_j$ have low dimension counterparts, $y_i$ and $y_j$, which have a similar probability distribution:\n",
    "\n",
    "$$\n",
    "q_{j|i}=\\frac{exp(-||y_i-y_j||^2)}{\\sum_{k\\neq i}exp(-||y_i-y_k||^2)}\n",
    "$$\n",
    "\n",
    "- $\\sigma$ is set to $\\frac{1}{(\\sqrt2)}$, so that term disappears in $q_{j|i}$  \n",
    "- If the data is perfectly projected $p_{j|i}=q_{j|i}.  \n",
    "- It won't be perfect, so we look to minimize the difference, which is called the Kullback-Leibler divergences. This is done via gradient descent. The cost function is:\n",
    "\n",
    "$$\n",
    "C = \\sum KL(P_i||Q_i) = \\sum_i \\sum_j p_{j|i} log \\frac{p_{j|i}} {q_{j|i}}\n",
    "$$\n",
    "\n",
    "#### SNE is hard to optimize, since \\sigma needs to be estimated (see paper).\n",
    "t-SNE instead employs a Student t-distribution, which leads to:\n",
    "\n",
    "$$\n",
    "q_{ij}=\\frac{(1+||y_i-y_j||^2)^{-1}}{\\sum_{k \\neq 1} (1+||y_k-y_l||^2)^{-1}}\n",
    "$$\n",
    "\n",
    "And the gradients become easier to calculate:\n",
    "\n",
    "$$\n",
    "\\frac{\\delta C}{\\delta y_i} = 4\\sum_{j}(p_{ij}-q_{ij})(y_i-y_j)(1+||y_i-y_j||^2)^{-1}\n",
    "$$\n",
    "\n",
    "\n",
    "The [linked paper](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf) above goes a bit more into the math if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempting to use PCA to visualize the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import datetime\n",
    "\n",
    "ts_start = datetime.datetime.now()\n",
    "pca = PCA(n_components=2).fit_transform(mnist_sample)\n",
    "\n",
    "ts_end = datetime.datetime.now()\n",
    "\n",
    "print(f'Completed in {ts_end-ts_start}')\n",
    "\n",
    "pca_df = pd.DataFrame(pca)\n",
    "pca_df.index = mnist_sample.index.tolist()\n",
    "pca_df.columns = ['component1', 'component2']\n",
    "\n",
    "pca_df = pd.concat([pca_df, mnist_sample_targets], axis=1)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(rc={'figure.figsize':(9,9)})\n",
    "sns.scatterplot(x='component1', y='component2', hue='class', data=pca_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PCA doesn't visually separate the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import datetime\n",
    "\n",
    "ts_start = datetime.datetime.now()\n",
    "tsne_model = TSNE(n_components=2, \n",
    "            init='random', \n",
    "            learning_rate='auto', \n",
    "            perplexity=30)\n",
    "\n",
    "tsne = tsne_model.fit_transform(mnist_sample)\n",
    "\n",
    "ts_end = datetime.datetime.now()\n",
    "\n",
    "print(f'Completed in {ts_end-ts_start}')\n",
    "\n",
    "tsne_df = pd.DataFrame(tsne)\n",
    "tsne_df.index = mnist_sample.index.tolist()\n",
    "tsne_df.columns = ['component1', 'component2']\n",
    "\n",
    "tsne_df = pd.concat([tsne_df, mnist_sample_targets], axis=1)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(rc={'figure.figsize':(9,9)})\n",
    "sns.scatterplot(x='component1', y='component2', hue='class', data=tsne_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import datetime\n",
    "\n",
    "ts_start = datetime.datetime.now()\n",
    "tsne = TSNE(n_components=2, \n",
    "            init='random', \n",
    "            learning_rate='auto', \n",
    "            perplexity=5).fit_transform(mnist_sample)\n",
    "\n",
    "ts_end = datetime.datetime.now()\n",
    "\n",
    "print(f'Completed in {ts_end-ts_start}')\n",
    "\n",
    "tsne_df = pd.DataFrame(tsne)\n",
    "tsne_df.index = mnist_sample.index.tolist()\n",
    "tsne_df.columns = ['component1', 'component2']\n",
    "\n",
    "tsne_df = pd.concat([tsne_df, mnist_sample_targets], axis=1)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(rc={'figure.figsize':(9,9)})\n",
    "sns.scatterplot(x='component1', y='component2', hue='class', data=tsne_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cluster size isn't meaningful. \n",
    "- May change shape given hyperparameter settings.  \n",
    "- Cluster closeness may not be meaningful.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Machine Learning with Python 3rd Edition, Chapter 5](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch05)  \n",
    "- [Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow, Chapter 8](https://github.com/ageron/handson-ml2/blob/master/08_dimensionality_reduction.ipynb)\n",
    "- [t-SNE Wiki](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)  \n",
    "- [t-SNE Papers by Laurens van der Maaten](https://lvdmaaten.github.io/tsne/)\n",
    "- [What, Why and How of t-SNE](https://towardsdatascience.com/what-why-and-how-of-t-sne-1f78d13e224d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

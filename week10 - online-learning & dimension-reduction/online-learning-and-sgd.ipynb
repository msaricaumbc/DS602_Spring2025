{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking/Online Learning and Stochastic/Mini-Batch Gradient Descent\n",
    "\n",
    "\n",
    "## Agenda  \n",
    "- Chunking  \n",
    "- Stochastic gradient descent.    \n",
    "- Working with larger datasets via online learning.  \n",
    "- Simple NLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "Sometimes reading the entire file into memory isn't possible. However, you might be able to summarize the data by reading it in pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dfUrl = 'https://raw.githubusercontent.com/msaricaumbc/DS_data/427906167b1370c0367c0650cc0fc465d0498d4b/ds602/Advertising.csv'\n",
    "\n",
    "df = pd.read_csv(dfUrl, chunksize=5)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using the `chunksize` option, we created an iterator for the file instead of the typical DataFrame.\n",
    "\n",
    "We can read over the chunks to get the data in pieces (i.e., chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chunck in df:\n",
    "#     print(chunck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = dict()\n",
    "features = ['TV', 'radio', 'newspaper', 'sales']\n",
    "records = 0\n",
    "\n",
    "i = 1\n",
    "# each chunk will be 5 rows.\n",
    "for chunk in df:\n",
    "    for feature in features:\n",
    "        channels[feature] = channels.get(feature, 0) + chunk[feature].sum()\n",
    "        records += chunk.shape[0]\n",
    "    print(f'Chunk {i} read. Contains {chunk.shape[0]}. Total records read: {records}.')\n",
    "    i += 1\n",
    "        \n",
    "channels = pd.Series(channels)/records\n",
    "\n",
    "channels[['TV', 'radio', 'newspaper']].plot.barh()\n",
    "plt.title('Average of Advertising Spend by Channel', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We could also use this for mining very large text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def download_file(file_name, url):\n",
    "    res = request.urlopen(url)\n",
    "    with open(file_name,'wb') as file:\n",
    "        file.write(res.read())\n",
    "        \n",
    "def unzip(file_name, path='./'):\n",
    "    # opening the zip file in READ mode \n",
    "    with ZipFile(file_name, 'r') as zip: \n",
    "        # printing all the contents of the zip file \n",
    "        zip.printdir() \n",
    "\n",
    "        # extracting all the files \n",
    "        print('Extracting all the files now...') \n",
    "        zip.extractall(path = path) \n",
    "        print('Done!')\n",
    "        \n",
    "download_file('imdb.zip', 'https://github.com/msaricaumbc/DS_data/blob/master/ds602/imdb2.zip?raw=true')\n",
    "unzip('imdb.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_filename = 'imdb.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = pd.read_csv(imdb_filename, chunksize=1)\n",
    "next(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a word cloud while only importing 1 row at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "words = dict()\n",
    "\n",
    "im = pd.read_csv(imdb_filename, chunksize=50)\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "for chunk in im:\n",
    "    r = chunk['review'].str.split(' ').tolist()[0]\n",
    "    for w in r:\n",
    "        words[w.lower()] = words.get(w.lower(), 0) + 1\n",
    "\n",
    "en = datetime.datetime.now()\n",
    "el = en - st\n",
    "\n",
    "print(f'Completed in {el}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Length of new dictionary: {len(words.keys()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the stopwords from our frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sw in list(stop):\n",
    "    words.pop(sw, None)\n",
    "    \n",
    "print(f'Length of new dictionary: {len(words.keys()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Could also use this to in conjunction with mini-batches for training or scoring large datasets with limited computing resources and to inform some of the feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What if your data is so large that your models take hours to train?\n",
    "- All of the models we've talked about so far use `batch` learning, which means the entire dataset is needed to update the parameters at each iteration.  \n",
    "- Really large datasets make this unfeasible.  \n",
    "- An alternative is `stochastic gradient descent (SGD)` or `mini-batch gradient descent`.  \n",
    "- `SGD` is also referred to as iterative or `online learning` since it learns after 1 observation.  \n",
    "- `mini-batch` learns after a batch of observations, something more than 1 and less than the number of observations in the dataset.\n",
    "\n",
    "#### Difference from `Gradient Descent`\n",
    "- Instead of updating the weights after each iteration, we update the weights based on the `cumulative sum of errors` over all the examples.  \n",
    "\n",
    "$$\n",
    "\\vartriangle w = \\eta \\sum_{i}(y^{(i)}-\\phi (z^{(i)}))x^{(i)}\n",
    "$$\n",
    "\n",
    "- $\\eta$ is the learning rate.  \n",
    "- `epochs` total number of times we are going to pass through the entire dataset (`outer for loop`), i.e., iterations.    \n",
    "- `batches` number of observations we'll use to update the errors each time (`inner for loop`). For `SGD` this will be 1.    \n",
    "- This is an approximation of gradient descent.  \n",
    "- Generally will converge faster because we are updating the weights more often.  \n",
    "- Since this is updated after after sample error, the error surface will be noisey.  \n",
    "- While the surface is noisey, it can break out of local minima easier.  \n",
    "- The trade-off to the ability to break out of local minimas is that it will can have trouble settling due to the variation that will now be in the loss curves.    \n",
    "- The learning rate, $\\eta$, is usually replaced by an adaptive learning rate, which usually helps reach the closer to the convergence point gradient descent would reach, i.e., the learning rate will generally decrease as epochs and iterations continue in order to better fine tune. This also helps with making sure it settles down. Usually this is implemented something like this:\n",
    "\n",
    "$$\n",
    "\\frac{t_0}{epochs+t_1}\n",
    "$$\n",
    "\n",
    "\n",
    "- We'll want to reshuffle the data so the training examples are independently, identically distributed after each epoch (full training cycle) to prevent recurring cycles that could influence learning.  \n",
    "- Another big benefit to `SGD` is that it allows us to stream observations into the model for training purposes, i.e., we'd never actually need to have all of the training data in memory for the purposes of modeling. If you data is say for example 10Gb and you only have 4Gb of memory on your machine, you wouldn't be able to load the entire dataset.\n",
    "\n",
    "### Simple example of `SGD`\n",
    "`Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow, page 125.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyprind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sample data that we'll eventually want to run a regression on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = 2 * np.random.rand(1000,1)\n",
    "y = 4 + 3 * X + np.random.randn(1000,1)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the constant term to $X$\n",
    "Remember you might need to do this depending on the software you are using!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((1000,1)), X]\n",
    "X_b[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple implementation of stochastic gradient descent for regression\n",
    "- Go through the dataset 25,000 times. \n",
    "- Update parameters after each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_epochs = 25000\n",
    "m = 1\n",
    "t0, t1 = 5, 50\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "# initialization to random numbers\n",
    "# recall this will cause the results to differ each time unless we freeze the seed\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        \n",
    "        # shuffling the dataset\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        \n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta -= eta * gradients\n",
    "        \n",
    "sgd_intercept, sgd_slope = theta\n",
    "\n",
    "print(f'Intercept: {float(sgd_intercept[0]):.2f}')\n",
    "print(f'Slope: {float(sgd_slope[0]):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch\n",
    "- Go through the dataset 500 times. \n",
    "- Update parameters after 50 observations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_epochs = 500\n",
    "m = 100\n",
    "t0, t1 = 5, 50\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "# initialization to random numbers\n",
    "# recall this will cause the results to differ each time unless we freeze the seed\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        \n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        \n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta -= eta * gradients\n",
    "        \n",
    "sgd_minibatch_intercept, sgd_minibatch_slope = theta\n",
    "print(f'Intercept: {float(sgd_minibatch_intercept[0]):.2f}')\n",
    "print(f'Slope: {float(sgd_minibatch_slope[0]):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to batch model\n",
    "- All observations at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = LinearRegression(fit_intercept=False).fit(X_b,y)\n",
    "lm_intercept = lm.coef_[0][0]\n",
    "lm_slope = lm.coef_[0][1]\n",
    "\n",
    "print(f'Intercept: {lm_intercept:.2f}')\n",
    "print(f'Slope: {lm_slope:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Not exact, but pretty close. The trade-off in performance relative to computation will be worthwhile for very large datasets. Mini-batch will be closer to batch results generally speaking.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np    \n",
    "\n",
    "def abline(intercept, slope, color, label):\n",
    "    \"\"\"Plot a line from slope and intercept\"\"\"\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, color, label=label)\n",
    "    \n",
    "    \n",
    "plt.scatter(X, y)\n",
    "abline(sgd_intercept, sgd_slope, 'r-', 'sgd')\n",
    "abline(sgd_minibatch_intercept, sgd_minibatch_slope, 'b-', 'mini_batch')\n",
    "abline(lm_intercept, lm_slope, 'g-', 'lr')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD) in scikit-learn\n",
    "- [Linear Regression $\\rightarrow$ LinearRegression:SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)  \n",
    "- [Various Classifiers $\\rightarrow$ SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier)  \n",
    "\n",
    "#### SGDClassifier\n",
    "The `loss` parameter controls the type of classifier you can use. The following is from the documentation:\n",
    "\n",
    ">The possible options are `hinge`, `log`, `modified_huber`, `squared_hinge`, `perceptron`, or a regression loss: `squared_error`, `huber`, `epsilon_insensitive`, or `squared_epsilon_insensitive`.\n",
    "<br><br>The `log` loss gives logistic regression, a probabilistic classifier. `modified_huber` is another smooth loss that brings tolerance to outliers as well as probability estimates. `squared_hinge` is like hinge but is quadratically penalized. `perceptron` is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description.  \n",
    "\n",
    "Use `.partial_fit` to utilize online/out-of-core learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_lm = SGDRegressor(fit_intercept=False,\n",
    "                      alpha=0, \n",
    "                      eta0=0.01, \n",
    "                      power_t=.05, \n",
    "                      learning_rate='adaptive'\n",
    "                     )\n",
    "\n",
    "samples = X_b.shape[0]\n",
    "random_idx = np.random.choice(range(1000), size=1000, replace=False)\n",
    "\n",
    "X_b = X_b[random_idx]\n",
    "y = y[random_idx]\n",
    "\n",
    "coefs = list()\n",
    "mses = list()\n",
    "\n",
    "for i in range(samples):\n",
    "    sgd_lm = sgd_lm.partial_fit(X_b[i:i+1], y[i:i+1].ravel())\n",
    "\n",
    "    b = sgd_lm.coef_[0]\n",
    "    m = sgd_lm.coef_[1]\n",
    "    coefs.append((b, m))\n",
    "    \n",
    "    yhat = sgd_lm.predict(X_b)\n",
    "    mse = ((y - yhat)**2).mean()\n",
    "    mses.append(mse)\n",
    "\n",
    "plt.plot(range(samples), [x[0] for x in coefs])\n",
    "plt.plot(range(samples), [x[1] for x in coefs])\n",
    "plt.legend(['Intercept', 'Slope'])\n",
    "plt.xlabel('Example')\n",
    "plt.ylabel('Weight')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(samples), mses)\n",
    "plt.xlabel('Example')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Parameter curve won't be as smooth as with batch updates.  \n",
    "> See the noise in the loss curve that we haven't seen in batch gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Larger Data\n",
    "- Sentiment analysis is a type of `natural language processing`.  \n",
    "- We'll be trying to predict whether the text is positive or negative about the movie. The movie being reviewed is unknown.  \n",
    "- We'll need to:\n",
    "    - Clean the data.  \n",
    "    - Extract features.  \n",
    "    - Train a `batch` model.  \n",
    "    - Compare to an `online` model.  \n",
    "    - Look at extracting topic vectors.  \n",
    "    \n",
    "Data source is from [Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).](https://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_name = 'imdb.csv'\n",
    "\n",
    "df = pd.read_csv(file_name, encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words\n",
    "We'll use a `bag-of-words` model for the prediction. Recall we've introduced this in earlier examples, such as the `spam` detection examples.\n",
    "\n",
    "`Bag-of-words` will let us represent the text reviews as simple numerical vectors.  \n",
    "\n",
    "- Create a `vocabulary` of unique tokens (i.e., words).  \n",
    "- Construct feature vector, with the counts of how often the word appears. These will mostly be sparse vectors, since each review will only use a small percentage of the unique words across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "\n",
    "docs = np.array(['hello there bill the ', \n",
    "                  'the ravens defense is terrible',\n",
    "                  'the traffic on 695 is silly',\n",
    "                  'the sun was shining today',\n",
    "                  'my daughter keeps me up to all hours of the night'\n",
    "                 ])\n",
    "\n",
    "bag = count.fit_transform(docs)\n",
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The above shows the indixes of where the words appear in the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Stores a sparse matrix since most of the documents only contain a small percentage of the overall vocabulary.\n",
    "\n",
    "However, you can covert to a normal array, if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bag.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Term Frequencies\n",
    "Values are also called raw term frequencies, $tf(t,d)$ - number of times a term, $t$, appears in a document, $d$.\n",
    "\n",
    "#### Note\n",
    "- Order of the words `is not` accounted for. The same vector can be created from `hello there bill` and `there bill hello`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document sparseness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(5), bag.toarray().mean(axis=1))\n",
    "plt.ylabel('Percent of Features $=0$')\n",
    "plt.xlabel('Sample of Documents, each bar is a document.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $N-$grams\n",
    "A way to add weight to the sequencing of tokens, though it explodes the size of the feature matrix if you use multiple $N-$grams.\n",
    "\n",
    "`the sun is shining`: 1-gram $\\rightarrow$ 'the', 'sun', 'is', 'shining'  \n",
    "`the sun is shining`: 2-gram $\\rightarrow$ 'the sun', 'sun is', 'is shining'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues with text data\n",
    "- Not all tokens are words, e.g., punctuation, random numbers, ...  \n",
    "- Misspelled words - really need a spell-checking API, or hopefully these are rare. \n",
    "- Synonyms, generic and business-specific.  \n",
    "    - Create a mapping dictionary, e.g., {'title 2':['disability', 'title ii', 'disabled worker'}\n",
    "   \n",
    "- Not all words are meaningful, e.g., 'the', sometimes referred to as `stop words`. These can be removed using common lists.   \n",
    "- Words may not be common across a language, but are across the documents, e.g., some type of business specific word or maybe the name of a product in product reviews. We can use `term frequency-inverse document frequency`. This reduces the weights of words that occur frequency across documents.  \n",
    "\n",
    "$$\n",
    "tf-idf(t,d)=tf(t,d) \\times idf(t,d), \\space where \\space idf(t,d)=log\\frac{1 + n_d}{1+df(d,t)}\n",
    "$$\n",
    "\n",
    "In the above $n_d$ is the total number of documents, $df(d,t)$ is the documents that contain $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "tfidf = tfidf.fit_transform(bag)\n",
    "\n",
    "print(tfidf.toarray().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Last 50 characters of first review:\\n')\n",
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Contains HTML tags and punctuation. Some punctuation might be meaningful, e.g., emojis.\n",
    "\n",
    "We can create a cleaning function, that could be applied to training and test data. We aren't using any data specific to the test set, so this doesn't cause leakage. We'll do this with regular expressions, which can best be described as:\n",
    "\n",
    "<img src='./diagrams/cat.jpeg' style='width: 500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expressions\n",
    "- Very powerful, but takes time to build complex ones.  \n",
    "- See [Python's re package for lite version of documentation](https://docs.python.org/3/library/re.html).  \n",
    "- For our purpose, we just want to remove the tags and standardize the text, however, regular expressions can be used to find positions in the text and extract complex features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    # find tags and replace with empty string\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    \n",
    "    # find emoticons\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    \n",
    "    # return the cleaned text\n",
    "    text = (re.sub(r'[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-',''))\n",
    "    return text\n",
    "\n",
    "examples = ['Hello <Br> I liked your hat :)', \n",
    "            'What a relief! That is great news',\n",
    "            'Bill, thanks - I appreciate it'\n",
    "           ]\n",
    "\n",
    "for ex in examples:\n",
    "    print(preprocessor(ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about `hat` versus `hats`?\n",
    "In many cases the plural forms of words don't provide additional semantic meaning. It is accepted practice to form these variations to reduce the dimensionality of the data. There are several methods, including [`stemming`](https://en.wikipedia.org/wiki/Stemming) and [`lemmatization`](https://en.wikipedia.org/wiki/Lemmatisation). We'll use `stemming` since it's simplier and less computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "tokenizer_porter('the hats were great. this hat was amazing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dictionaries\n",
    "In business contexts, there might be multiple ways to convey the same concept, e.g., in disability programs, `disability` might be written as `t2 dib`, `title ii disabled worker`, or `oasdi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businessTerms = {'t2 dib':'disability',\n",
    "                 'title ii disabled worker':'disability', \n",
    "                 'oasdi':'disability'\n",
    "                }\n",
    "\n",
    "ex = ['I have to file for t2 dib.',\n",
    "      'I have to file for title ii disabled worker.',\n",
    "      'I have to file for oasdi.'\n",
    "     ]\n",
    "\n",
    "print(*ex, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for e in ex:\n",
    "    for k in businessTerms.keys():\n",
    "        r = re.search(k, e)\n",
    "        if r:\n",
    "            print(e)\n",
    "            print(r)\n",
    "            print(re.sub(k, businessTerms[k], e))\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Not aware these types of replacements that would be useful for the IMDB data, but this could be useful in an actual business application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "Stop words are generally semantically meaningless and can usually be removed for analyses that are using frequency counts.\n",
    "\n",
    "If you haven't used `nltk` before, you might need to run:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "print('Sample stop words:\\n')\n",
    "print(*stop[:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text, stopwords):\n",
    "    return [porter.stem(word) for word in text.split() if word not in stopwords]\n",
    "\n",
    "tokenizer_porter('the hats were great. this hat was amazing.', stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> See [translate from string to remove punctuation](https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Batch` Classifier for `imdb`\n",
    "Create a training and test set. The data was already shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:24999, 'review'].values\n",
    "y_train = df.loc[:24999, 'sentiment'].values\n",
    "\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "print(f'Training samples: {X_train.shape[0]:,}')\n",
    "print(f'Test samples: {X_test.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we'd want to use `grid search`, but that could take an hour to run depending on the amount of parameters we are searching over. We'll take a couple liberties and see how long it takes to train one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents='ascii', \n",
    "                        lowercase=True,\n",
    "                        stop_words='english',\n",
    "                        analyzer='word'\n",
    "                       )\n",
    "\n",
    "p = Pipeline([('vect', tfidf),\n",
    "              ('lm', LogisticRegression(C=10, solver='liblinear'))\n",
    "             ])\n",
    "\n",
    "p = p.fit(X_train, y_train)\n",
    "\n",
    "en = datetime.datetime.now()\n",
    "\n",
    "el = en - st\n",
    "print(f'Time to fit model: {el}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_score =  p.score(X_test, y_test)\n",
    "print(f'Test accuracy: {p_score:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Took about 4-5 seconds to run, but what if it were 25 million records instead of 25,000? Depending on your computer, 25 million records may not even fit into your RAM...\n",
    "\n",
    "#### Online learning to the rescue!\n",
    "- We can stream the documents into the model with `SGD` or `mini-batch`.  \n",
    "- We will need to add some processing utilities to clean some of the streaming data. But since we are working with one or a couple dozen records at a time, that isn't a lot of processing overhead.\n",
    "\n",
    "The tokenizer should look familiar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    \n",
    "    text = (re.sub(r'[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-',''))\n",
    "    tokens = [w for w in text.split() if w not in stop]\n",
    "    return tokens\n",
    "\n",
    "tokenizer_test = 'hi! there bill, it is great!!!! to see you :)'\n",
    "print(tokenizer(tokenizer_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll need a function to stream the documents (reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label\n",
    "            \n",
    "stream = stream_docs(file_name)\n",
    "next(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need another function to get `mini batches` of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y\n",
    "\n",
    "get_minibatch(stream_docs(file_name), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` requires holding the data in-memory, but `HashingVectorizer` can be used.\n",
    "\n",
    "From [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer):\n",
    "\n",
    "This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.\n",
    "\n",
    "This strategy has several advantages:\n",
    "\n",
    "- it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory.  \n",
    "- it is fast to pickle and un-pickle as it holds no state besides the constructor parameters.  \n",
    "- it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.  \n",
    "\n",
    "There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):\n",
    "\n",
    "- there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model.  \n",
    "- there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this is rarely an issue if n_features is large enough (e.g. 2 ** 18 for text classification problems).  \n",
    "- no IDF weighting as this would render the transformer stateful.\n",
    "\n",
    "`tl:dr`: it lets you use term frequencies without holding the entire data in-memory, but you cannot use `tf-idf` or look at the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# tokenizer is the function we created a few cells above\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer\n",
    "                        )\n",
    "\n",
    "clf = SGDClassifier(loss='log_loss', random_state=1)\n",
    "doc_stream = stream_docs(path=file_name)\n",
    "\n",
    "print('Ready to model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "\n",
    "iter_no = 50\n",
    "\n",
    "pbar = pyprind.ProgBar(iter_no)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(iter_no):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=500)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=25000)\n",
    "X_test = vect.transform(X_test)\n",
    "\n",
    "t_scr = clf.score(X_test, y_test)\n",
    "print(f'Test accuracy: {t_scr:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Very similar performance using `online modeling`. If the training data was actually really huge, `online` learning may have had to be our default. For our `online` model, only `500` documents were loaded into memory at any one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "# exclude words that occur in more than 10% of the documents\n",
    "# include most common 5000 words\n",
    "count = CountVectorizer(stop_words='english', max_df=.1, max_features=5000)\n",
    "\n",
    "X = count.fit_transform(df['review'].values)\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, \n",
    "                               random_state=10, \n",
    "                               learning_method='batch'\n",
    "                              )\n",
    "\n",
    "X_topics = lda.fit_transform(X)\n",
    "\n",
    "en = datetime.datetime.now()\n",
    "\n",
    "el = en-st\n",
    "\n",
    "print(f'Time to complete: {el}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word importances for each of the 5,000 words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5\n",
    "\n",
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Topic {topic_idx+1}')\n",
    "    print(','.join([feature_names[i] for i in topic.argsort()[:-top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You would need to convert these into logical categories to present to your boss. You would want to check the topics match the text, since LDA can extract topics that aren't much stronger than random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy = X_topics[:, 0].argsort()[::-1]\n",
    "for iter_idx, movie_idx in enumerate(comedy[:3]):\n",
    "    print(f'\\nComedy {iter_idx+1}:')\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Topic {topic_idx+1}')\n",
    "    print(','.join([feature_names[i] for i in topic.argsort()[:-top_words - 1:-1]]))\n",
    "    \n",
    "    topic = X_topics[:, topic_idx].argsort()[::-1]\n",
    "    \n",
    "    for iter_idx, movie_idx in enumerate(topic[:3]):\n",
    "        print(f'- {iter_idx+1}:')\n",
    "        print(df['review'][movie_idx][:300], '...')\n",
    "        \n",
    "    print ('------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Seem reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings and Resources\n",
    "- [scikit-learn out of core learning](https://scikit-learn.org/0.15/modules/scaling_strategies.html)\n",
    "- [Google's tutorial on regular expressions](https://developers.google.com/edu/python/regular-expressions)  \n",
    "- [Python's regular expression documentation](https://docs.python.org/3/library/re.html)  \n",
    "- [Standord NLP](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)  \n",
    "- [NLTK Book - Natural Language Processing with Python](https://www.nltk.org/book/)  \n",
    "- [Latent Dirichlet Allocation paper by Blei and el.](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)  \n",
    "- [IncrementialPCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html)  \n",
    "- [Text Mining with R Online Textbook](https://www.tidytextmining.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm imdb.zip imdb.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

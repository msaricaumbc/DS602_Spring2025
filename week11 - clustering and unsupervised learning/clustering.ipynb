{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering/Unsupervised Learning/Semi-supervised Learning\n",
    "\n",
    "## Agenda\n",
    "- k-means. \n",
    "- Unsupervised learning extensions.  \n",
    "- Agglomative clustering.  \n",
    "- Density based clustering.  \n",
    "- Topic modeling.\n",
    "\n",
    "## Unsupervised Learning\n",
    "Clustering and data mining\n",
    "\n",
    "## Semi-supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Following [*Python Machine Learning 3rd Edition, Chapter 11*](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch11) and [*Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow, Chapter 9*](https://github.com/ageron/handson-ml2/blob/master/09_unsupervised_learning.ipynb).\n",
    "\n",
    "<img src='./diagrams/unsupervised-clustering.png' style='height: 500px'>\n",
    "\n",
    "[Image source Machine Learning with Python 3rd Edition](https://github.com/rasbt/python-machine-learning-book-3rd-edition)\n",
    "\n",
    "- We need clustering when we don't have class labels, making this an unsupervised problem.  \n",
    "- When we need to discover a latent or hidden structure. We may have a hypothesis on what this structure might be in logical terms, but in reality most of the algorithms are going to be looking at maximizing distance between centroids.  \n",
    "- Clusters should contain examples that are more similar to each other than in different clusters.  \n",
    "\n",
    "## When might we need to use clustering?\n",
    "- Determine documents that are similar to each other.  \n",
    "- Group songs when we don't have information on their genre.  \n",
    "- Generation tasks, where should this new sample belong?  \n",
    "- Customer segmentation for marketing campaigns.  \n",
    "- Determine outliers in higher dimension datasets.  \n",
    "- Need to flag high-risk credit card transactions.  \n",
    "\n",
    "## General practices\n",
    "- Most clustering algorithms use `distance` in some manner. Therefore, it is a best practice to `scale` your data prior to using these algorithms. Since distance will be influenced by absolute magnitudes, it is crucial you don't skip this step (and you shouldn't be skipping it for other modeling tasks either!).  \n",
    "- To visually inspect the clustering, a standard practice is to use dimensionality reduction (e.g., `principle component analysis`) to remove some of the background noise in the data.  \n",
    "- Another standard practice is to use dimensionality reduction to compress the feature space to 2 or 3 dimensions in order to visualize the separation in the clusters - though the preceived distance in 2 dimensions may not be reflect of the absolute or relative difference in the full $d$-dimensional space.  \n",
    "- We'll need a subject matter expert (SME) to review our results and determine the purity of the clusters. If you had some labeled examples you could get a sense of purity using those, but in many cases you'll need some type of feedback loop with reviewers.\n",
    "\n",
    "## Primary types of Clustering\n",
    "- Prototype-based (e.g., k-means) clustering. \n",
    "- Hierarchical (agglomerative) clustering.  \n",
    "- Density-based clustering.  \n",
    "- Graph-based clustering.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Means Clustering\n",
    "- Type of `prototype-based` clustering - it needs a prototype or model for each cluster that is usually a centroid (mean/center) or medoid (median).  \n",
    "- Identifies spherical shapes well.  \n",
    "- Every example gets assigned a cluster, even if it is a true outlier.  \n",
    "- Biggest downside is you need to tell the algorithm how many clusters there are...  \n",
    "- A bad decision for the number of clusters ($k$) and our performance will be poor.  \n",
    "\n",
    "Demonstrate on the `iris` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "iris_df = iris.data\n",
    "iris_label = iris.target\n",
    "\n",
    "iris_df[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Squeeze to 2-dimensions for visualizing\n",
    "We'll use PCA to reduce the dataset to 2-dimensions.  \n",
    "- Scale the data.    \n",
    "- Apply PCA to reduce to 2-dimensions.  \n",
    "- Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "iris_p = Pipeline([('scale', StandardScaler()),\n",
    "                   ('pca', PCA(n_components=2))\n",
    "                  ])\n",
    "\n",
    "iris_2d = iris_p.fit_transform(iris_df)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(iris_2d[:,0], iris_2d[:,1], 'ro', alpha=0.5)\n",
    "plt.title('iris in 2-dimensions', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We know there are 3 classes of `iris` but if we didn't have that info, we would probably assume there are 2 based on this plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Process with k-means\n",
    "- Pick $k$. \n",
    "- Pick $k$ centroids randomly and assign as the initial centers.  \n",
    "- Assign each example to the nearest cluster, based on `distance`.  \n",
    "- Move the centroids to the center of the examples that were assigned to it.  \n",
    "- Repeat until the centroids stop changing, either based on a tolerance or iteration threshold.  \n",
    "\n",
    "### Distance\n",
    "Usually this will be `squared Euclidean distance`, where $j$ is the $j$th dimension:\n",
    "\n",
    "$$\n",
    "d(x,y)^2 = \\sum_{j=1}^{m}(x_j - y_j)^2 = ||x-y||_2^2\n",
    "$$\n",
    "\n",
    "### Minimizing within-cluster errors\n",
    "Going to minimize the within-cluster errors using the `sum of squared errors`, leveraging `Euclidean distance`.\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i=1}^{n}{\\sum_{j=1}^{k}}w^{(i,j)}||x^{(i)}-\\mu^{(j)}||_2^2\n",
    "$$\n",
    "\n",
    "$\\mu^{j}$ is the $j$th centroid, $i$ is for the $i$-th example, $w^{(i,j)}$ is an indicator for belonging to the cluster:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "w^{(i,j)}\n",
    "=\n",
    "\\begin{cases}\n",
    "1, & \\text{if}\\ x^{(i)}\\in j \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters = 2, init='random', n_init='auto')\n",
    "km = km.fit(iris_2d)\n",
    "iris_cluster = km.predict(iris_2d)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def scatter(class_, color):\n",
    "    plt.scatter(iris_2d[iris_cluster == class_, 0], \n",
    "                iris_2d[iris_cluster == class_, 1], color=color,\n",
    "                alpha=0.5,\n",
    "                label= f'Cluster {class_}'\n",
    "               )\n",
    "\n",
    "scatter(0, 'blue')\n",
    "scatter(1, 'orange')\n",
    "\n",
    "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], color = 'red', marker='*')\n",
    "\n",
    "plt.title('iris in 2-dimensions', loc='left')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looked like it did a reasonable job.  \n",
    "- This was easy since there was so much separation between clusters.  \n",
    "\n",
    "### What if we tried 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "km = KMeans(n_clusters = 3, init='random', n_init='auto')\n",
    "km = km.fit(iris_2d)\n",
    "iris_cluster = km.predict(iris_2d)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def scatter(class_, color):\n",
    "    plt.scatter(iris_2d[iris_cluster == class_, 0], \n",
    "                iris_2d[iris_cluster == class_, 1], color=color,\n",
    "                alpha=0.5,\n",
    "                label= f'Cluster {class_}'\n",
    "               )\n",
    "\n",
    "scatter(0, 'blue')\n",
    "scatter(1, 'orange')\n",
    "scatter(2, 'red')\n",
    "\n",
    "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], color = 'black', marker='*')\n",
    "\n",
    "plt.title('iris in 2-dimensions', loc='left')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that this may not line up like supervised learning as assigned cluster number may not match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(iris_label, iris_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now what?\n",
    "- Might be useful to join back to the original data and see if there are commonalities among some of the features.\n",
    "- Pass it on to a SME to review.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Means++\n",
    "- Ordinary k-Means randomly sets the initial centroids, which could be very poor.  \n",
    "- Could lead to bad performance and/or slow convergence.  \n",
    "- Similar to cross-validation, we can run k-Means multiple times and pick the best in terms of the `sum of squared errors`.  \n",
    "- This has been proven to be more consistent and have better general performance in literature.\n",
    "\n",
    "### Basic Process:\n",
    "- Create a list, $M$, to store the $k$ centroids.  \n",
    "- Randomly choose first centroid, $\\mu^{(j)}$, from the examples and add to $M$.  \n",
    "- For examples not in $M$, find the minimum squared distance to the centroids in $M$.  \n",
    "- Randomly select the next centroid, $\\mu^{(p)}$ using a weighted sampling distribution and repeat above. The sampling distribution ensures instances farther away from existing centroids are more likely to be new centroids.\n",
    "- Run normal k-means.\n",
    "\n",
    "To use `init=k-means++` in the algorithm, which is actually the default.\n",
    "\n",
    "You can use specify default values for the initial centroids, but that would be probably pretty rare that you would have a guess for those, especially with higher dimension data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another k-Means example\n",
    "This will use a generated blob dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=2000, \n",
    "                  n_features=2, \n",
    "                  centers=10, \n",
    "                  cluster_std=2,\n",
    "                  shuffle=True,\n",
    "                  random_state=0\n",
    "                 )\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], alpha=.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What should be set for $k$?  \n",
    "We have two primary tools to determine a reasonable number to set $k$:\n",
    "- Elbow plot. \n",
    "- Silhouette plot.  \n",
    "\n",
    "### Elbow plot\n",
    "- Think of this as a type of grid search for clustering.  \n",
    "- We don't have evaluation metrics, e.g., recall, that we can look to maximize, so we need to use metrics intrinsic to the clusters and data.  \n",
    "- We use the within-cluster SSE (`inertia` or `distortion`) and look to see that reaches a convergence point. inertia_ is available after fitting a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "distortions = list()\n",
    "\n",
    "for i in range(1, 15):\n",
    "    km = KMeans(n_clusters=i, random_state=0, n_init='auto')\n",
    "    km = km.fit(X)\n",
    "    distortions.append(km.inertia_)\n",
    "    \n",
    "plt.plot(range(1, 15), distortions, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot `elbows` or `kinks` at around $6$, $7$ or $8$ so we'll use that for $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n = 8\n",
    "\n",
    "km = KMeans(n_clusters=n, random_state=0, n_init='auto')\n",
    "km = km.fit(X)\n",
    "pclusters = km.predict(X)\n",
    "\n",
    "for i in range(n):\n",
    "    plt.scatter(X[pclusters==i, 0], X[pclusters==i, 1], alpha=0.25)\n",
    "    \n",
    "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], color = 'black', marker='*')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Elbow method seemed to pick a reasonable number?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Plot\n",
    "Another method for chosing $k$. It's a measure of how tightly grouped the estimates within clusters.\n",
    "\n",
    "#### Steps\n",
    "- Calculate cluster cohesion, $a^{(i)}$, which is the average distance between a sample and all other samples in the cluster. The smaller, the tighter there are together.  \n",
    "- Calculate the cluster separation, $b^{(i)}$, which is the average distance between each sample in a cluster versus the examples in the nearest cluster.  \n",
    "- Calculate the silhouette, which is the distance between the cluster cohesion and the separation, divided by the greater of the two.\n",
    "\n",
    "$$\n",
    "s^{(i)} = \\frac{b^{(i)}-a^{(i)}}{max(b^{(i)}, a^{(i)})}\n",
    "$$\n",
    "\n",
    "Where $s\\in(-1,1)$\n",
    "- A value of $1$ indicates an ideal cluster, since the clusters are dissimilar from each other.  \n",
    "- A value of $0$ indicates the within and between separation is the same, so the clusters are similar to each other (we don't want this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "silhouettes = list()\n",
    "\n",
    "for i in range(2, 15):\n",
    "    km = KMeans(n_clusters=i, random_state=0, n_init='auto')\n",
    "    km = km.fit(X)\n",
    "    s = silhouette_score(X, km.labels_)\n",
    "    silhouettes.append(s)\n",
    "    \n",
    "plt.plot(range(2,15), silhouettes, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Probably would ignore $2$ since it probably is separating the top and bottom blob. It materially begins to decline after $6$ or $10$, so we'd likely use that for $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n = 10\n",
    "\n",
    "km = KMeans(n_clusters=n, random_state=0, n_init='auto')\n",
    "km = km.fit(X)\n",
    "pclusters = km.predict(X)\n",
    "\n",
    "silhouette_vals = silhouette_samples(X, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s_vals = pd.DataFrame(list(zip(silhouette_vals, km.labels_)))\n",
    "s_vals.columns = ['silhouette', 'cluster']\n",
    "s_vals = s_vals.sort_values(by=['cluster', 'silhouette'],\n",
    "                          ascending=[True, False]\n",
    "                          )\n",
    "s_vals['idx'] = pd.Series(range(0,2000))\n",
    "s_vals['cluster'] = s_vals['cluster'].astype(str)\n",
    "\n",
    "s_vals.groupby('cluster')['silhouette'].plot.density()\n",
    "plt.legend()\n",
    "plt.xlim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot the individual values by cluster to identify potential outliers.\n",
    "\n",
    "- Data points near $0$ are less like the examples in the cluster.  \n",
    "- Heavy density near $1$ indicates the cluster is grouping similar examples and is dissimilar from other clusters.\n",
    "\n",
    "You may also see silhouette plots like this:\n",
    "\n",
    "<img src='./diagrams/silhouette.png' style='width: 500px'>\n",
    "\n",
    "[Image source: Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow, Chapter 9](https://github.com/ageron/handson-ml2/blob/master/09_unsupervised_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can also use clustering for preprocessing\n",
    "Since this can be used to generate a single data point per example from many, this could be used for dimension reduction in addition to segmentation.\n",
    "\n",
    "Why it may help?\n",
    "- Noise reduction.  \n",
    "- Dimensionality reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "dX, dy = digits['data'], digits['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "dX_train, dX_test, dy_train, dy_test = train_test_split(dX, \n",
    "                                                        dy,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=100\n",
    "                                                       )\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "log_reg = log_reg.fit(dX_train, dy_train)\n",
    "\n",
    "log_reg_score = log_reg.score(dX_test, dy_test)\n",
    "print(f'Score: {log_reg_score:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we'll see using the clusters from k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "p = Pipeline([('sc', StandardScaler()),\n",
    "             ('kmeans', KMeans(n_clusters=100, n_init='auto')),\n",
    "              ('logreg', LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "p = p.fit(dX_train, dy_train)\n",
    "p_score = p.score(dX_test, dy_test)\n",
    "print(f'Using k-means preprocesser: {p_score:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You could use GridSearchCV to find the best number of clusters.  \n",
    "- We picked $100$ based on literature on this dataset, but you would want to search for the best values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Application: Semi-Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataStr = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "dataio = StringIO(dataStr)\n",
    "\n",
    "df = pd.read_csv(dataio, sep=\"\\t\")\n",
    "\n",
    "df.sort_values(by=df.columns[1], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Height (inches)', 'Hair length (inches)']]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "ph = Pipeline([('sc', StandardScaler()),\n",
    "             ('kmeans', KMeans(n_clusters=n_clusters, n_init='auto'))])\n",
    "\n",
    "\n",
    "ph.fit(X)\n",
    "\n",
    "df['cluster'] = ph.predict(X)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph['kmeans'].cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dist = ph.transform(X)\n",
    "X_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "representative_idx = np.argmin(X_dist, axis=0)\n",
    "representative_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'] = None\n",
    "\n",
    "cluster=0\n",
    "for idx in representative_idx:\n",
    "#     print(idx)\n",
    "    print(df[idx:idx+1])\n",
    "    gender = input()\n",
    "    df['gender'] = [ gender if c == cluster else g  for c, g in zip(df['cluster'], df['gender'])]\n",
    "    cluster+=1\n",
    "    print('----------------------')\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (len(df) -  0 ) / len(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to digits dataset\n",
    "Let's pretend we only had 50 labeled images for the digits dataset.  \n",
    "- Something like this is somewhat common, especially with text or images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "log_reg.fit(dX_train, dy_train)\n",
    "\n",
    "all_n_score = log_reg.score(dX_test, dy_test)\n",
    "print(f'Accuracy with all training examples: {all_n_score:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labeled = 50\n",
    "\n",
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "log_reg.fit(dX_train[:n_labeled], dy_train[:n_labeled])\n",
    "\n",
    "small_n_score = log_reg.score(dX_test, dy_test)\n",
    "print(f'Accuracy with only 50 training examples: {small_n_score:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Accuracy much lower than with the full training dataset.\n",
    "\n",
    "#### What if we create labels from the clusters?\n",
    "We label other instances that are really close to centroids after running k-Means?\n",
    "\n",
    "First, we'll need to determine clusters, then determine which labeled instance is closest to the centroid.\n",
    "\n",
    "[Taken from *Hands on Machine Learning with Scikit-Learn, Keras & TensorFlow pages 253-254*](https://github.com/ageron/handson-ml2/blob/master/09_unsupervised_learning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 50\n",
    "\n",
    "kmeans = KMeans(n_clusters = k, n_init='auto')\n",
    "X_digits_dist = kmeans.fit_transform(dX_train)\n",
    "X_digits_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digit with smallest distance\n",
    "representative_digit_idx = np.argmin(X_digits_dist, axis=0)\n",
    "representative_digit_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_representative_digits = dX_train[representative_digit_idx]\n",
    "X_representative_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_train[representative_digit_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dy_train[representative_digit_idx]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These will be images that are nearest to the cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 4))\n",
    "for index, X_representative_digit in enumerate(X_representative_digits):\n",
    "    plt.subplot(k // 10, 10, index + 1)\n",
    "    plt.imshow(X_representative_digit.reshape(8, 8), cmap=\"binary\", interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If we didn't have the labels, we would need to manually look at these and label these manually. Since we have the labels, we'll skip that manual exercise, but be aware it would be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_representative_digits = dy_train[representative_digit_idx]\n",
    "y_representative_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "log_reg = log_reg.fit(X_representative_digits, y_representative_digits)\n",
    "\n",
    "new_scr = log_reg.score(dX_test, dy_test)\n",
    "print(f'Accuracy with only 50 representative training examples: {new_scr:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Significant improvement in accuracy! Can we do better?\n",
    "Now we'll iteratively create labels all the instances in the cluster and the closest digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_\n",
    "# print(dX_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(kmeans.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(kmeans.labels_).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_propagated = np.empty(len(dX_train), dtype=np.int32)\n",
    "\n",
    "for i in range(k):\n",
    "    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]\n",
    "\n",
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "log_reg = log_reg.fit(dX_train, y_train_propagated)\n",
    "\n",
    "new_scr = log_reg.score(dX_test, dy_test)\n",
    "print(f'Accuracy with propagation: {new_scr:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Another performance boost!\n",
    "\n",
    "#### What if we only took the closest examples?\n",
    "We'll try only propagating to the examples in the closest 20th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_closest = 20\n",
    "\n",
    "X_cluster_dist = X_digits_dist[np.arange(len(dX_train)), kmeans.labels_]\n",
    "for i in range(k):\n",
    "    in_cluster = (kmeans.labels_ == i)\n",
    "    cluster_dist = X_cluster_dist[in_cluster]\n",
    "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
    "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
    "    X_cluster_dist[in_cluster & above_cutoff] = -1\n",
    "    \n",
    "partially_propagated = (X_cluster_dist != -1)\n",
    "X_train_partially_propagated = dX_train[partially_propagated]\n",
    "y_train_partially_propagated = y_train_propagated[partially_propagated]\n",
    "\n",
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\n",
    "\n",
    "new_scr = log_reg.score(dX_test, dy_test)\n",
    "print(f'Accuracy with propagation: {new_scr:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative (hierarchical) clustering\n",
    "Another clustering method builds the clusters as parts of a tree and let's us visualize it via a `dendrogram`.\n",
    "\n",
    "This can be a useful technique if you expect some type of structure, e.g., you are trying to cluster on recipes. You would expect (hope) the french recipes get grouped together, the Chinese recipes grouped together, ... and then the various variations within those grouped in the terminal nodes.\n",
    "\n",
    "### Basic Steps\n",
    "- Compute a distance matrix for the examples (this could be very large).  \n",
    "- Each example starts as its own cluster.  \n",
    "- Merge the two closest clusters based on distance.  \n",
    "- Update the similarity (linkage) matrix.  \n",
    "- Repeat until one cluster remains.  \n",
    "\n",
    "This is a bottom-up mode, since we start with the individual examples and slowly group them into more and more heterogenous clusters until we are back to the entire dataset.\n",
    "\n",
    "- You can use Euclidean distance or a custom method of your choosing. \n",
    "- Since this is based on distance; you'll want to standardize your features first!  \n",
    "- Linkage is how the clusters are compared for similarity to one another:\n",
    "\n",
    "<img src='./diagrams/linkage.png' style='width: 500px'>\n",
    "\n",
    "[Image source Machine Learning with Python 3rd Edition Page 368](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchial Clustering with the Wine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wine = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/ensembles/wine.data', header=None)\n",
    "\n",
    "wineColumns = ['Type',\n",
    "'Alcohol',\n",
    "'Malic acid',\n",
    "'Ash',\n",
    "'Alcalinity of ash',\n",
    "'Magnesium',\n",
    "'Total phenols',\n",
    "'Flavanoids',\n",
    "'Nonflavanoid phenols',\n",
    "'Proanthocyanins',\n",
    "'Color intensity',\n",
    "'Hue',\n",
    "'OD280/OD315 of diluted wines',\n",
    "'Proline'\n",
    "]\n",
    "\n",
    "wine.columns = wineColumns\n",
    "\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the data and run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# pretending we don't know the type\n",
    "wine_X = wine.iloc[:, 1:]\n",
    "\n",
    "wine_X = StandardScaler().fit_transform(wine_X)\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=5, \n",
    "                             metric='euclidean', \n",
    "                             linkage='complete',\n",
    "                             compute_distances=True\n",
    "                            )\n",
    "\n",
    "labels = ac.fit_predict(wine_X)\n",
    "\n",
    "print('Clustering completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can plot the dendrogram\n",
    "[`plot_dengrogram` taken from scikit-learn example.](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "    \n",
    "plt.figure(figsize=(12,5))\n",
    "plot_dendrogram(ac, truncate_mode='lastp')\n",
    "plt.xlabel('Number of Examples {$Example ID$ or $(Examples)$}')\n",
    "plt.ylabel('Distance')\n",
    "plt.title('Dendrogram for Wine Agglomerative Clustering', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can be useful in identifying outliers or seeing how close examples are to one another.  \n",
    "- Still need to review features to determine what the clusters represent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density-based Spatial Clustering of Applications with Noise (DBSCAN)\n",
    "\n",
    "<img src='diagrams/dbscan1.png' style='width: 500px'>\n",
    "\n",
    "[source](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)\n",
    "\n",
    "- Density-based clustering - essentially identifying areas of continuous high density. \n",
    "- No assumptions about whether the clusters are spherical or not.  \n",
    "- No assumptions about hierarchies.  \n",
    "- Can identity points that are outliers.  \n",
    "\n",
    "### Each Example is Assigned a Label \n",
    "- Core point: if it's sufficiently close to neighbors defined by a minimum number (`MinPts`) falling within a radius, $\\epsilon$. Points within $\\epsilon$ are also considered its neighborhood.  \n",
    "- Border point: on the outside border of the cluster boundary, defined by being within the radius ($\\epsilon$) of a core point but lacking the minimum number of neighbors.  \n",
    "- Noise point: think of these as outliers that aren't attached to any clusters.  \n",
    "\n",
    "### Basic Process\n",
    "- Create clusters for each core point or connected group. They are considered connected if they are within $\\epsilon$.  \n",
    "- Assign the border points.\n",
    "\n",
    "<img src='./diagrams/dbscan.png' style='width: 600px'>\n",
    "\n",
    "[Image source Machine Learning with Python 3rd Edition Page 377](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch11)\n",
    "\n",
    "<img src='diagrams/dbscan_p.gif' style='width: 600px'>\n",
    "\n",
    "[source](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)\n",
    "\n",
    "\n",
    "### Biggest Advantages\n",
    "- Identifies noise/outliers. \n",
    "- No assumptions on structure.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with Half-Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=0.05, random_state=0)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how k-Means handles this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=2, n_init='auto')\n",
    "km = km.fit(X)\n",
    "\n",
    "pclusters = km.predict(X)\n",
    "\n",
    "for i in range(n):\n",
    "    plt.scatter(X[pclusters==i, 0], X[pclusters==i, 1], alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> k-Means assumes spherical clusters, so this isn't going to work...  \n",
    "\n",
    "### Now with DBSCAN\n",
    "> Note: there isn't a `.predict()` method for DBSCAN since it's meant to identify clusters based on the current dataset. If you needed predictions, you could use the outputs and use with KMeans. See pages 257-258 of *Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=0.2, min_samples=5, metric='euclidean')\n",
    "pclusters = db.fit_predict(X)\n",
    "labs = db.labels_\n",
    "ulabs = np.unique(labs)\n",
    "\n",
    "for i in ulabs:\n",
    "    plt.scatter(X[pclusters==i, 0], X[pclusters==i, 1], alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DBSCAN was able to identity the areas of continously density, and thus, the half moons.  \n",
    "- So for data of arbitrary shapes, this will probably perform better than k-Means or hierarchical clustering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=0.05, min_samples=5, metric='euclidean')\n",
    "pclusters = db.fit_predict(X)\n",
    "labs = db.labels_\n",
    "ulabs = np.unique(labs)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "for i in ulabs:\n",
    "    plt.scatter(X[pclusters==i, 0], X[pclusters==i, 1], alpha=0.25)\n",
    "plt.legend(ulabs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Setting the radius too small, and nearly everything could be an outlier ($-1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cons\n",
    "- Vulnerable to curse of dimensionality (not unique, not something to be aware of).  \n",
    "- Need to determine $\\epsilon$ and the minimum number of points.  \n",
    "- If the density is very uneven, finding reasonable choices for $\\epsilon$ and the minimum points could be challenging.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Clustering Algorithms\n",
    "- [Balance Iterative Reducing and Clustering using Hierarchies (BIRCH)](https://en.wikipedia.org/wiki/BIRCH): designed for very large datasets. Similar to k-means and traditionally has similar results.    \n",
    "- [Mean-Shift](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html):  Starts with a circle around each example and computes mean of instances within that circle, then centers so the it is centered on the mean. Shifts to areas of higher density. Circles in similar places are assigned to the same cluster. Very computationally complicated and not suitable for bigger datasets.     \n",
    "- [Affinity](https://en.wikipedia.org/wiki/Affinity_propagation): Voting based.  \n",
    "- [Spectral clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html): Uses similarity matrix and simple embedding and then uses k-means (or another clustering algorithm).  \n",
    "- [Gaussian Mixtures](https://scikit-learn.org/stable/modules/mixture.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at similar people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import urllib.request as urllib2\n",
    "\n",
    "def get_data():\n",
    "    r = urllib2.urlopen(\"https://github.com/msaricaumbc/DS_data/blob/master/ds602/people_wiki.zip?raw=true\").read()\n",
    "    file = ZipFile(BytesIO(r))\n",
    "    people_wiki_csv = file.open(\"people_wiki.csv\")\n",
    "    people = pd.read_csv(people_wiki_csv)\n",
    "    return people\n",
    "\n",
    "people = get_data()\n",
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.drop('URI', inplace=True, axis=1)\n",
    "people.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(stop_words='english', max_features=10000)\n",
    "corpus = vec.fit_transform(people.text)\n",
    "corpus = corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus\n",
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df = pd.DataFrame(corpus, columns=vec.get_feature_names_out())\n",
    "vector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = people.query('name == \"Joe Biden\"')\n",
    "person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus[person.index[0]]\n",
    "vector_df.iloc[person.index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df.iloc[person.index[0]][vector_df.iloc[person.index[0]] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity([corpus[person.index[0]]], corpus)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people['similarity'] = similarity[0]\n",
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.sort_values(by='similarity', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in people.sort_values(by='similarity', ascending=False)[:5].text:\n",
    "    print(i[:500])\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "\n",
    "def get_similar_people(name, top=5):\n",
    "    person = people.query(f'name == \"{name}\"')\n",
    "    if len(person)==0:\n",
    "        print('no match')\n",
    "        return\n",
    "    \n",
    "    similarity = cosine_similarity([corpus[person.index[0]]], corpus)\n",
    "    people['similarity'] = similarity[0]\n",
    "    return people.sort_values(by='similarity', ascending=False)[:top]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_people('Tom Hanks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in get_similar_people('Aamir Khan').text:\n",
    "    print(i[:500])\n",
    "    print('------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief interlude for topic modeling\n",
    "40+ thousand biographies is way too many documents for individuals to read. We can use `Latent Dirichlet Allocation` to reduce the text into topic vectors so we understand what is being stated (generally) in the documents with relatively little overhead.\n",
    "\n",
    "In a nutshell, it finds words that frequently appear together across documents. It uses two matrices (1) `document-to-topic` and (2) `word-to-topic` that when multiplied together reproduce the bag-of-words matrix.  Like PCA, you `need to define the number of topics`.\n",
    "\n",
    "Starting with the word frequencies of the `wikipedia` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "# exclude words that occur in more than 10% of the documents\n",
    "# include most common 5000 words\n",
    "count = CountVectorizer(stop_words='english', max_df=.1, max_features=5000)\n",
    "\n",
    "X = count.fit_transform(people['text'].values)\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, \n",
    "                               random_state=10, \n",
    "                               learning_method='batch'\n",
    "                              )\n",
    "\n",
    "X_topics = lda.fit_transform(X)\n",
    "\n",
    "en = datetime.datetime.now()\n",
    "\n",
    "el = en-st\n",
    "\n",
    "print(f'Time to complete: {el}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word importances for each of the 5,000 words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5\n",
    "\n",
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Topic {topic_idx+1}')\n",
    "    print(','.join([feature_names[i] for i in topic.argsort()[:-top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You would need to convert these into logical categories to present to your boss. You would want to check the topics match the text, since LDA can extract topics that aren't much stronger than random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Topic {topic_idx+1}')\n",
    "    print(','.join([feature_names[i] for i in topic.argsort()[:-top_words - 1:-1]]))\n",
    "    \n",
    "    topic = X_topics[:, topic_idx].argsort()[::-1]\n",
    "    \n",
    "    for iter_idx, movie_idx in enumerate(topic[:3]):\n",
    "        print(people['name'][movie_idx], people['text'][movie_idx][:300], '...')\n",
    "        \n",
    "    print ('------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Seem reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings\n",
    "- [Stanford k-Means Notes](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html)  \n",
    "- [Dendrogram Gallery](https://www.r-graph-gallery.com/dendrogram.html)  \n",
    "- [DBSCAN Paper](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf)  \n",
    "- [Fast Algorithms for Association Rule Mining](http://www.vldb.org/conf/1994/P487.PDF)  \n",
    "- [Non-negative Matrix Factorization Wiki](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization)  \n",
    "- [Non-negative Matrix Factorization Example](https://medium.com/logicai/non-negative-matrix-factorization-for-recommendation-systems-985ca8d5c16c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

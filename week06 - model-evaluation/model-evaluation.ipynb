{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Design, Model Evaluation, and Grid Search\n",
    "\n",
    "Following (mostly) Python Machine Learning 3rd (Raschka) Chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "- Variability in splitting data.  \n",
    "- Holdout method.  \n",
    "- Cross-validation.  \n",
    "- Learning curves.  \n",
    "- Validation curves.  \n",
    "- Grid search.  \n",
    "- Class imbalance.  \n",
    "\n",
    "\n",
    "## Resources\n",
    "[Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning, Raschka](https://arxiv.org/abs/1811.12808)\n",
    "<br>[SMOTE](https://arxiv.org/pdf/1106.1813.pdf)\n",
    "<br>[Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation](https://arxiv.org/abs/2010.16061)\n",
    "<br>[scikit-learn Model Selection](https://scikit-learn.org/stable/model_selection.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Test Split\n",
    "This is how we've set-up our examples so far. We'll use the [breast cancer dataset from scikit-learn.](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "bc = load_breast_cancer()\n",
    "\n",
    "X = bc['data']\n",
    "y = bc['target']\n",
    "X_names = bc['feature_names']\n",
    "\n",
    "bcDf = pd.concat([pd.DataFrame(X),pd.DataFrame(y)], axis=1)\n",
    "bcDf.columns = list(X_names) + ['target']\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "bcDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the numerical summaries of this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcDf.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See if there are any interesting distributions or data issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcDf.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No missing values, so no need to impute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "bcDf.hist(figsize=(14,14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doesn't look like there are any obvious data quality issues.  \n",
    "- All numerical, but on different scales, so we'll need to standardize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(bcDf.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are a lot of correlated features.  [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) \n",
    "- We are going to try using [principal components](https://en.wikipedia.org/wiki/Principal_component_analysis) to extract uncorrelated features.  \n",
    "    - We'll take more about exactly what this is doing later.  \n",
    "- We are going to extract 5, which will reduce the feature space from 30 dimensions to 5. (Arbitrary decision)\n",
    "    - This is technically a suite of hyperparameters we are introducing, but this is a well researched dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Gameplan\n",
    "- Split the data into training/test. \n",
    "- Create feature processing pipeline. \n",
    "    - Standardize features. \n",
    "    - Extract 5 PCA components. (arbitrary decision; could evaluate)\n",
    "- Fit a Logistic Regression model.  \n",
    "- Evaluate accuracy (relatively balanced dataset so accuracy is okay).  \n",
    "\n",
    "<img src='./diagrams/06_01.png' style='width: 700px;'>\n",
    "\n",
    "[Image source: Python Machine Learning 3rd Edition, Raschka](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch06)\n",
    "\n",
    "#### Benefits of pipelines\n",
    "- Chains together feature processing and fitting steps.  \n",
    "- No separate feature transformation fits for test data.  \n",
    "- Plays nice with many of the more robust evaluation options.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_splits(X, y):\n",
    "    return train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "X_train, X_test, y_train, y_test = create_splits(X, y)\n",
    "\n",
    "print(f'Training sample: {X_train.shape[0]:,}')\n",
    "print(f'Test sample: {X_test.shape[0]:,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def generate_estimates(x, y, comp=5):\n",
    "    \n",
    "    modeling_pipeline = Pipeline([\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('pca', PCA(n_components=comp)),\n",
    "        ('model', LogisticRegression(penalty=None))\n",
    "    ])\n",
    "\n",
    "    return modeling_pipeline.fit(x, y)\n",
    "\n",
    "m = generate_estimates(X_train, y_train)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make our predictions on the test set and determine performance estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test_pred = m.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def generate_probs(X, model):\n",
    "    return model.predict_proba(X)[:, 1]\n",
    "\n",
    "def generate_roc(y, probs):\n",
    "    fpr, tpr, _ = roc_curve(y, probs)\n",
    "    return fpr, tpr\n",
    "    \n",
    "fpr_test, tpr_test = generate_roc(y_test, generate_probs(X_test, m))\n",
    "fpr_train, tpr_train = generate_roc(y_train, generate_probs(X_train, m))\n",
    "\n",
    "plt.plot(fpr_test, tpr_test,'-r')\n",
    "plt.plot(fpr_train, tpr_train,'-b')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(['Test','Training'])\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"training roc score\", roc_auc_score(y_train, generate_probs(X_train, m)))\n",
    "print(\"test roc score\", roc_auc_score(y_test, generate_probs(X_test, m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is our performance estimate potentially flawed?\n",
    "- How representative is the test dataset?  \n",
    "- Why did we choose 20% to hold for the test set?  \n",
    "- Feature sets in our partitions follow the same distribution?  \n",
    "- Did we try any additional settings or parameter combinations?  \n",
    "- We don't really have a baseline to compare our results against.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What type of variation can we expect in the test data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 1000\n",
    "trainingMeans = []\n",
    "testMeans = []\n",
    "\n",
    "i = 0\n",
    "while i < samples:\n",
    "    X_train, X_test, y_train, y_test = create_splits(X, y)\n",
    "    trainingMeans.append(np.mean(y_train))\n",
    "    testMeans.append(np.mean(y_test))\n",
    "    i += 1\n",
    "\n",
    "plt.hist(trainingMeans)\n",
    "plt.xlim(0.4, 0.8)\n",
    "plt.title('Percent of Positive Classes in Training Data')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(testMeans)\n",
    "plt.xlim(0.4, 0.8)\n",
    "plt.title('Percent of Positive Classes in Test Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Smaller dataset will have more vulnerability to sampling issues.  \n",
    "- What if our data set was bigger?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# population for our illustration\n",
    "population = np.random.binomial(1, 0.5, size=1000000)\n",
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(population).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's draw samples at varying sample sizes to see how representative sample means would be from a large distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "reps = 100\n",
    "sample_sizes = [5, 10, 50, 100, 250, 500, 1000, 2500, 5000, 10000, 100000, 200000]\n",
    "means = defaultdict(list)\n",
    "\n",
    "for rep in range(reps):\n",
    "    for ss in sample_sizes:\n",
    "        means[ss].append(np.random.choice(a=population, size=ss, replace=False).mean())\n",
    "        \n",
    "meansDf = pd.DataFrame.from_dict(means)\n",
    "meansDf #.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meansDf.describe().iloc[3:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meansDf.quantile(0.50)\n",
    "plt.plot(meansDf.quantile(0.50))\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0.05, 0.1, 0.25, 0.50, 0.75, 0.9, 0.95, 1]:\n",
    "    if i == 0.5:\n",
    "        plt.plot(meansDf.quantile(i), '-b')\n",
    "    else:\n",
    "        plt.plot(meansDf.quantile(i), '--r')\n",
    "plt.xscale('log')\n",
    "plt.title('Percentiles of Sampling Distribution')\n",
    "plt.xlabel(f'Sample Sizes {sample_sizes}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Small samples are going to be much more at risk for sampling variations.  \n",
    "- It is not advisable to use a simple training/test split for small datasets.  \n",
    "    - It's very likely your test set won't mirror your training.  \n",
    "    - Neither may mirror the \"population\".  \n",
    "- For larger datasets, choosing a smaller percentage for test can be okay.  \n",
    "    - The number of samples held will be hopefully very large, and have less risk to sampling variation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rascha on Choosing an appropriate ratio for partitioning a dataset into training and test dates\n",
    "From page 124 of *Python Machine Learning 3rd Edition, Raschka*:\n",
    "> If we are dividing a dataset into training and test datasets, we have to keep in mind that we are withholding valuable information that the learning algorithm could benefit from.<br><br>Thus, we don't want to allocate too much information to the test set.<br><br>However, the smaller the test set, the more inaccurate the estimation of the generalization error.<br><br>Dividing a dataset into training and test datasets is all about balancing this tradeoff.<br><br>...<br><br>Moreover, instead of discarding the allocated test data after model training and evaluation, it is a common practice to retrain a classifier on the entire dataset, as it can improve the predictive performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strengths\n",
    "- Quick guage of performance.  \n",
    "- Okay if looking for quick interpretability.\n",
    "\n",
    "\n",
    "## Weaknesses\n",
    "- Prone to overfitting and not ideal for tuning hyperparameters. \n",
    "- Many don't partition the training into training and validation, which isn't a good practice.  \n",
    "- There could be other issues with the splits that affect the comparability and would lead to poor results in the real world. The performance estimate is going to be sensitive to how the partition was done. \n",
    "\n",
    "## Other issues to be aware\n",
    "- Is the historical data representative of the current data generating process?  \n",
    "- Have there been structure shifts in the data over time? (leakage)  \n",
    "- Rare event issues and sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better: Holdout Method\n",
    "This is for larger datasets, where we don't need to worry about sampling variation risk. We are going to pretend the breast cancer data is \"big\" enough for this to be a valid approach.\n",
    "\n",
    "- Split the data into training and test.  \n",
    "> **Develop and train our models using the training partition and estimate general performance on the test set.**  \n",
    "**PRETEND THE TEST SET DOESN'T EXIST WHILE MODELING OR PERFORMING PREPROCESSING. IDEAS WHY?**\n",
    "- Split the training into training and validation.  \n",
    "> **If we train our models on the entire training dataset, we are going to be at risk for overfitting? Why?** \n",
    "- Training data is where we'll be creating models and those will be evaluated on the validation data. The test set is for final validation and check that we didn't overfit on the validation set.\n",
    "\n",
    "<img src='./diagrams/training-validation-test-data-set.png' style='width: 600px;'>\n",
    "\n",
    "[Image source: Raschka, Chapter 6, Page 196](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_holdout_splits(X, y):\n",
    "    x_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = create_holdout_splits(X, y)\n",
    "\n",
    "print(f'Training sample: {X_train.shape[0]:,}')\n",
    "print(f'Validation sample: {X_val.shape[0]:,}')\n",
    "print(f'Test sample: {X_test.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare and evaluate models\n",
    "Let's see how the PCA model to a model that uses the full feature set.\n",
    "\n",
    "### Create pipeline we can use for feature transforms and prediction.\n",
    "- Might as well check our assumption on using 5 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def generate_estimates(use_pca = False, comp=None):\n",
    "    if use_pca:\n",
    "        modeling_pipeline = Pipeline([\n",
    "            ('scaling', StandardScaler()),\n",
    "             ('pca', PCA(n_components=comp)),\n",
    "             ('model', LogisticRegression(penalty=None))\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        modeling_pipeline = Pipeline([\n",
    "            ('scaling', StandardScaler()),\n",
    "             ('model', LogisticRegression(penalty=None))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return modeling_pipeline\n",
    "\n",
    "m_rawFeatures = generate_estimates(use_pca=False).fit(X_train, y_train)\n",
    "print('raw features:', m_rawFeatures)\n",
    "\n",
    "m_pca_models = {}\n",
    "for i in range(1,6):\n",
    "    m_pca_models[i] = generate_estimates(use_pca=True, comp=i).fit(X_train, y_train)\n",
    "    print(f'pca{i}:', m_pca_models[i])\n",
    "    \n",
    "print('Models fitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_val_rawFeatures = m_rawFeatures.predict(X_val)\n",
    "\n",
    "print('Using raw features:')\n",
    "print(confusion_matrix(y_val, y_val_rawFeatures))\n",
    "print('----------------------')\n",
    "y_val_pca = {}\n",
    "for i in range(1,6):\n",
    "    y_val_pca[i] = m_pca_models[i].predict(X_val)\n",
    "    print(f'Using PCA{i}:')\n",
    "    print(confusion_matrix(y_val, y_val_pca[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks like the PCA model performed better and maybe we didn't need all 5 of those components.\n",
    "\n",
    "### Check the ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def generate_probs(X, model):\n",
    "    return model.predict_proba(X)[:, 1]\n",
    "\n",
    "def generate_roc(y, probs):\n",
    "    fpr, tpr, _ = roc_curve(y, probs)\n",
    "    return fpr, tpr\n",
    "    \n",
    "fpr_val_rawFeatures, tpr_val_rawFeatures = generate_roc(y_val,\n",
    "                                                        generate_probs(X_val, model=m_rawFeatures))\n",
    "\n",
    "\n",
    "fpr_val_pca = {}\n",
    "tpr_val_pca = {}\n",
    "for i in range(1,6):\n",
    "    fpr_val_pca[i], tpr_val_pca[i] = generate_roc(y_val,\n",
    "                                          generate_probs(X_val, model=m_pca_models[i]))\n",
    "\n",
    "\n",
    "plt.plot(fpr_val_rawFeatures, tpr_val_rawFeatures,'-r')\n",
    "for i in range(1,6):\n",
    "    plt.plot(fpr_val_pca[i], tpr_val_pca[i],'-b')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(['Raw Features (No Regularization)','PCA'])\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks like the ROC curves overlap themselves for each component.  \n",
    "- Using PCA performed significantly better in the validation data.  \n",
    "- Now check to see if it continues in the test data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_test_rawFeatures, tpr_test_rawFeatures = generate_roc(y_test,\n",
    "                                                        generate_probs(X_test, model=m_rawFeatures))\n",
    "print(f'Using raw features:')\n",
    "print(confusion_matrix(y_test, m_rawFeatures.predict(X_test)))\n",
    "print(roc_auc_score(y_test, generate_probs(X_test, m_rawFeatures)))\n",
    "print('\\n')\n",
    "\n",
    "# create false/true positive rate curves\n",
    "fpr_test_pca = {}\n",
    "tpr_test_pca = {}\n",
    "for i in range(1,6):\n",
    "    conf_matrix = m_pca_models[i].predict(X_test)\n",
    "    print(f'Using PCA{i}:')\n",
    "    print(confusion_matrix(y_test, conf_matrix))\n",
    "    \n",
    "    probs = generate_probs(X_test, model=m_pca_models[i])\n",
    "    fpr_test_pca[i], tpr_test_pca[i] = generate_roc(y_test, probs)\n",
    "    print(roc_auc_score(y_test, probs))\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "# plot the ROC curve\n",
    "plt.plot(fpr_test_rawFeatures, tpr_test_rawFeatures,'-r')\n",
    "\n",
    "for i in range(1,6):\n",
    "    plt.plot(fpr_test_pca[i], tpr_test_pca[i],'-b')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(['Raw Features (No Regularization)','PCA'])\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks like the performance was consistent with what we observed in the validation dataset.\n",
    "- Which model should be choose in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation - Best option for small datasets\n",
    "\n",
    "<img src='./diagrams/06_03.png' style='width: 600px;'>\n",
    "\n",
    "[Image source: Python Machine Learning 3rd Edition, Figure 6.3](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch06)\n",
    "\n",
    "#### Basic mechanics:  \n",
    "- Split the data into training and test. \n",
    "- The training data will be divided into *k* folds.  \n",
    "    - Each folder will use a different partition for the validation data.  \n",
    "- The models will be run on each fold.  \n",
    "- Performance estimate will be taken with the median or mean score.  \n",
    "    - Need to define what metric you are optimizing toward.  \n",
    "\n",
    "You could do this with a loop, but scikit-learn has this built-in.\n",
    "\n",
    "Let's get our data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_splits(X, y):\n",
    "    return train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "X_train, X_test, y_train, y_test = create_splits(X, y)\n",
    "\n",
    "print(f'Training sample: {X_train.shape[0]:,}')\n",
    "print(f'Test sample: {X_test.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-up the same pipeline as before and run it through the function\n",
    "Things to consider:  \n",
    "- Need an estimator (classifer/regression), which a pipeline satisfies if the last step is a model.  \n",
    "- This accepts multiple metrics, so you'll need to determine which one is most appropriate.\n",
    "\n",
    "[List of metrics accepted](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def generate_estimates(comp=5):\n",
    "    \n",
    "    modeling_pipeline = Pipeline([\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('pca', PCA(n_components=5)),\n",
    "        ('model', LogisticRegression(penalty=None))\n",
    "    ])\n",
    "\n",
    "    return modeling_pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "clf = generate_estimates()\n",
    "cv_results = cross_validate(clf, X_train, y_train, \n",
    "                            scoring=['accuracy', 'recall', 'precision', 'f1_macro', 'roc_auc'], cv=5)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this return?\n",
    "- How long it took the model to fit. This will be more important with larger datasets and more complex models.  \n",
    "- How long it took the model to score the validation set in the fold.  \n",
    "- The metrics.  \n",
    "\n",
    "It returns an array so you can look at the distribution and/or central tendency of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in cv_results.keys():\n",
    "    print(f'{k}: {cv_results[k].mean():.4f} (+/- {cv_results[k].std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solved the sampling variation issue, now can compare between models with better certainty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def generate_estimates(use_pca = False, comp=None):\n",
    "    if use_pca:\n",
    "        modeling_pipeline = Pipeline([\n",
    "            ('scaling', StandardScaler()),\n",
    "            ('pca', PCA(n_components=5)),\n",
    "            ('model', LogisticRegression(penalty=None))\n",
    "        ])\n",
    "    else:\n",
    "        modeling_pipeline = Pipeline([\n",
    "            ('scaling', StandardScaler()),\n",
    "            ('model', LogisticRegression())\n",
    "        ])\n",
    "\n",
    "    return modeling_pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "pca_clf = generate_estimates(use_pca=True, comp=5)\n",
    "nopca_clf = generate_estimates()\n",
    "\n",
    "pca_cv_results = cross_validate(pca_clf, X_train, y_train, scoring=['accuracy', 'recall', 'precision', 'f1_macro', 'roc_auc'], cv=5)\n",
    "pca_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nopca_cv_results = cross_validate(nopca_clf, X_train, y_train, scoring=['accuracy', 'recall', 'precision', 'f1_macro', 'roc_auc'], cv=5)\n",
    "nopca_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = pca_cv_results['test_accuracy'].mean()\n",
    "a2 = nopca_cv_results['test_accuracy'].mean()\n",
    "\n",
    "plt.plot(pca_cv_results['test_accuracy'])\n",
    "plt.plot(nopca_cv_results['test_accuracy'])\n",
    "plt.legend([f'PCA (mean={a1:.2f})',f'No PCA (mean={a2:.2f})'])\n",
    "plt.title('Accuracy on Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = pca_cv_results['test_f1_macro'].mean()\n",
    "a2 = nopca_cv_results['test_f1_macro'].mean()\n",
    "\n",
    "plt.plot(pca_cv_results['test_f1_macro'])\n",
    "plt.plot(nopca_cv_results['test_f1_macro'])\n",
    "plt.legend([f'PCA (mean={a1:.2f})',f'No PCA (mean={a2:.2f})'])\n",
    "plt.title('F1-Score on Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What looks like the better option?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation not just for classification tasks\n",
    "- Same concept for regression, the scoring metrics will be different.  \n",
    "- Recall our California housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "ch = fetch_california_housing()\n",
    "ch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_housing = pd.DataFrame(ch['data'], columns=list(ch['feature_names']))\n",
    "X_housing.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_housing = ch['target']\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def generate_estimates():\n",
    "    \n",
    "    modeling_pipeline = Pipeline([\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('model', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    return modeling_pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "reg = generate_estimates()\n",
    "cv_results = cross_validate(reg, X_housing, y_housing, scoring=['r2'], cv=10)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cv_results['test_r2'])\n",
    "plt.title('$R^2$ Distribution from Cross-validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Looks like the parameters being learned are highly influenced by the training data for California housing. (overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But what about setting hyperparameters?\n",
    "How to determine how many components we may want to use?\n",
    "\n",
    "### General comments on hyperparameters:\n",
    "- These aren't learned by the model, they are selected by the analyst.  \n",
    "- You need to learn the best combination by experimenting across the search space.  \n",
    "- If you have a large dataset you can evaluate these in the validation dataset.  \n",
    "- If you have a smaller dataset you can evaluate these using cross validation.  \n",
    "\n",
    "Consider how large your search space can be:\n",
    "- For regularization, the $C$ parameter in Logistic Regression could span from near zero to approaching infinity.  \n",
    "- For hyperparameters with that type of bounds, you'll likely want to start with a handful of values that are spacing on a logarithmic scale, e.g. $C \\in (0.001, 0.01, 0.1, 1, 10, 100, 1000)$.  \n",
    "- If you have other hyperparameters, e.g., the specific solver for Logistic Regression, you'll need to evaulate each solver at each regularization level:  \n",
    "\n",
    "$$\\begin{equation*}\n",
    "PE_{solver,C} = \n",
    "\\begin{pmatrix}\n",
    "PE_{newton-cg,0.001} & PE_{newton-cg,0.01} & \\cdots & PE_{newton-cg,1000} \\\\\n",
    "PE_{lbfgs,0.001} & PE_{lbfgs,0.01} & \\cdots & PE_{lbfgs,1000} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "PE_{saga,0.001} & PE_{saga,0.01} & \\cdots & PE_{saga,1000} \n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "- So you if you have 7 different regularization strengths and 5 different solvers, you'll be running $7*5=35$ models. If you use 10 cross-validation folds, that will be 350 models (hence smaller datasets for CV).  \n",
    "- And making it more complicated, you may have multiple metrics (e.g., precision vs. recall).  \n",
    "\n",
    "> When you find the \"best\" set of hyperparameters, you'll want to explore the nearby space in more detail.\n",
    "\n",
    "- If you find $C=10$ as the \"best\", you might want to try another round with $C \\in (7,8,9,10,11,12,13)$.  \n",
    "\n",
    "> Finding the \"perfect\" set of hyperparameters is likely impossible. The search space is generally going to be too large.\n",
    "\n",
    "- Random search of Bayesian hyperparameter optimization can help in those situations.\n",
    "- In random search you'll provide a distribution of values instead of discrete values.  \n",
    "- Optimization-based searches will try to make intelligent choices based on past explorations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Curves\n",
    "- You can use these to visualize seeing the differences in different parameters.  \n",
    "- scikit-learn uses a varient on k-fold cross-validation to plot the distribution of metrics for different parameter values.  \n",
    "- We can plot the range of accuracies we observe for many folds across different settings of the parameter.\n",
    "\n",
    "This is using the breast cancer dataset. We are going to vary $C$ to see its effect. Some of this code is borrowed from page 205-206 of Python Machine Learning 3rd Edition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lg_pipe = modeling_pipeline = Pipeline([\n",
    "         ('scaler', StandardScaler()),\n",
    "         ('pca', PCA(n_components=5)),\n",
    "         ('logreg', LogisticRegression(penalty='l2'))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "training_scores, test_scores = validation_curve(estimator=lg_pipe, X=X_train, y=y_train,\n",
    "                                               param_name='logreg__C', \n",
    "                                               param_range=param_range, \n",
    "                                               cv=10)\n",
    "\n",
    "train_mean = np.mean(training_scores, axis=1)\n",
    "train_std = np.std(training_scores, axis=1)\n",
    "\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(param_range, train_mean, color='blue', marker='o', label='Training Accuracy')\n",
    "plt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha=0.2, color='blue')\n",
    "\n",
    "plt.plot(param_range, test_mean, color='orange', marker='o', label='Test Accuracy')\n",
    "plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha=0.2, color='orange')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$C$')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What does this suggest as a parameter for $C$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "\n",
    "<img src='./diagrams/grid_search_workflow.png' style='width: 500px;'>\n",
    "\n",
    "[Image source](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)\n",
    "\n",
    "Examples:\n",
    "- [Digits dataset](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py)  \n",
    "- [Text extraction](https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py)  \n",
    "- [Multiple metric example](https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py)\n",
    "\n",
    "On the breast cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lg_pipe = modeling_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "         ('pca', PCA(n_components=5)),\n",
    "         ('logreg', LogisticRegression(penalty='l2', solver='liblinear'))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "  {\n",
    "    'logreg__penalty': ['l1', 'l2'],\n",
    "    'logreg__C': [1, 10, 100, 1000], \n",
    "    'pca__n_components': [1,2,3,4,5,10,15]\n",
    "  }\n",
    " ]\n",
    "\n",
    "gcv_results = GridSearchCV(estimator=lg_pipe, param_grid=param_grid, scoring='accuracy')\n",
    "gcv_results = gcv_results.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This outputs a dictionary with the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_results.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And a series of summarys for the best fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_results.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_results.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After you find the \"best\" hyperparameters, you'll retrain your training data using those and then evaluate the test data using that model. There's an option in GridSearchCV to do this automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could use multiple estimators, but it'll get a little complicated, see below for example:\n",
    "\n",
    "```python\n",
    "   from sklearn.base import BaseEstimator\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "    class DummyEstimator(BaseEstimator):\n",
    "        def fit(self): pass\n",
    "        def score(self): pass\n",
    "        \n",
    "    # Create a pipeline\n",
    "    pipe = Pipeline([('clf', DummyEstimator())]) # Placeholder Estimator\n",
    "    \n",
    "    # Candidate learning algorithms and their hyperparameters\n",
    "    search_space = [{'clf': [LogisticRegression()], # Actual Estimator\n",
    "                     'clf__penalty': ['l1', 'l2'],\n",
    "                     'clf__C': np.logspace(0, 4, 10)},\n",
    "                    \n",
    "                    {'clf': [DecisionTreeClassifier()],  # Actual Estimator\n",
    "                     'clf__criterion': ['gini', 'entropy']}]\n",
    "    \n",
    "    \n",
    "    # Create grid search \n",
    "    gs = GridSearchCV(pipe, search_space)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Wrap-up\n",
    "\n",
    "<img src='./diagrams/model-eval-conclusions.jpg' style='width: 600px;'>\n",
    "\n",
    "[Image source: *Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning, Raschka*](https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unbalanced Classes\n",
    "\n",
    "Consider the credit from last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "credit = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/log_reg/Default.csv', index_col=0)\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remember we had interesting distributions with our numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.hist(bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit['balance2income'] = credit['balance']/credit['income']\n",
    "\n",
    "credit['balance2income'].hist(bins=100)\n",
    "plt.title('Balance-Income Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit['default'].value_counts().plot.barh()\n",
    "plt.title('Default are relatively rare...', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall we didn't have the best results with a straightforward logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Will try to add some additional features to capture some of the mixture distributions and the truncation of the credit balance.\n",
    "\n",
    "We can add these prior to processing since they are only considering the example row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit['balance_student_int'] = np.where(credit['student']=='Yes', credit['balance'], 0)\n",
    "credit['income_student_int'] = np.where(credit['student']=='Yes', credit['income'], 0)\n",
    "credit['zero_balance'] = np.where(credit['balance'] == 0, 'Yes', 'No')\n",
    "\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll do Cross Validation, but still need to split the data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "creditFeatures = [x for x in credit.columns if x != 'default']\n",
    "y = np.where(credit['default'] == 'Yes', 1, 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(credit[creditFeatures], y, test_size=0.20)\n",
    "\n",
    "print(f'Training examples: {X_train.shape[0]:,}')\n",
    "print(f'Test examples: {X_test.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a modeling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "nums = ['balance2income', 'balance', 'income', 'balance_student_int', 'income_student_int']\n",
    "ohes = ['student', 'zero_balance']\n",
    "\n",
    "processing_pipeline = ColumnTransformer(transformers=[\n",
    "    ('numscaling', StandardScaler(), nums),\n",
    "    ('dummys', OneHotEncoder(drop='first'), ohes)]\n",
    ")\n",
    "\n",
    "modeling_pipeline = Pipeline([\n",
    "    ('data_processing', processing_pipeline),\n",
    "    ('logreg', LogisticRegression())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditbase = modeling_pipeline.fit(X_train, y_train)\n",
    "base_p = creditbase.predict(X_test)\n",
    "base_pr = creditbase.predict_proba(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, base_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define search space\n",
    "> We'll try using weights to try to account for the unbalanced distribution of defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {\n",
    "    'logreg__class_weight': [None, 'balanced'], \n",
    "   'logreg__C':[0.01, 0.1, 1, 10, 100]}\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the experiment\n",
    "> Let's look for high-recall, since default could be very expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_results = GridSearchCV(estimator=modeling_pipeline, param_grid=param_grid, scoring='recall', refit=True)\n",
    "gcv_results = gcv_results.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_results.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_results.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine how this performs on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_testp = gcv_results.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_testp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did weighting help?\n",
    "- Recall went from 0.36 to 0.92 for detecting the default.  \n",
    "- Precision went from 0.69 to 0.17 for detecting the default.  \n",
    "> We detected more of the defaults, with the trade-off of more false-positives. Depending what the cost of those is - which you'd need to talk with a SME about - this might be a preferred model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "y_testpr = gcv_results.predict_proba(X_test)\n",
    "\n",
    "def generate_roc(y, probs):\n",
    "    fpr, tpr, _ = roc_curve(y, probs)\n",
    "    return fpr, tpr\n",
    "    \n",
    "fpr_wgt, tpr_wgt = generate_roc(y_test, y_testpr[:,1])\n",
    "fpr_base, tpr_base = generate_roc(y_test, base_pr[:,1])\n",
    "\n",
    "plt.plot(fpr_wgt, tpr_wgt,'-r')\n",
    "plt.plot(fpr_base, tpr_base,'--b')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(['Weighted Model','Unweighted Model'])\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Weighted model has essentially the same ROC curve as the unweighted model in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(creditbase['data_processing'].get_feature_names_out(),creditbase['logreg'].coef_[0])\n",
    "plt.barh(creditbase['data_processing'].get_feature_names_out(), gcv_results.best_estimator_['logreg'].coef_[0], alpha=0.5)\n",
    "plt.legend(['Unweighted Coefficients', 'Weighted Coefficients'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling and Generating new data\n",
    "\n",
    "### Resampling. \n",
    "If weighting isn't supporting, you can resample the training data.  \n",
    "- This gets complicated since you want to evaluate based on the original distribution.  \n",
    "- GridSearchCV won't support this well since the validation dataset is split from the training data.  \n",
    "- You could use loops for this, but you need to be careful to make sure the performance estimate is based on the `actual` distribution.\n",
    "\n",
    "### Generating new data.  \n",
    "- The more complicated, probably less ROI option, is generating new data.  \n",
    "-  Synthetic Minority Over-sampling Technique(SMOTE) is a popular technique, if needed. [See this paper for a description](https://arxiv.org/pdf/1106.1813.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is another library called `imbalanced-learn` that has methods specifically designed for these types of problems as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using classifiers to determine dataset bias \n",
    "- We shouldn't be able to predict whether an example in the training or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "\n",
    "iX_train, iX_test, iy_train, iy_test = train_test_split(iris['data'], \n",
    "                                                        iris['target'], \n",
    "                                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new label, whether included in the training or test set and that will be our new target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inTraining = np.ones((iX_train.shape[0], 1))\n",
    "inTest = np.zeros((iX_test.shape[0], 1))\n",
    "\n",
    "irisTarget = np.append(inTraining, inTest, axis=0).reshape(-1)\n",
    "irisTraining = np.append(iX_train, iX_test, axis=0)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(class_weight='balanced')\n",
    "clf = clf.fit(irisTraining, irisTarget)\n",
    "preds = clf.predict(irisTraining)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(irisTarget, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dumdum = DummyClassifier(strategy='uniform')\n",
    "dumdum = dumdum.fit(irisTraining, irisTarget)\n",
    "dumPreds = dumdum.predict(irisTraining)\n",
    "\n",
    "confusion_matrix(irisTarget, dumPreds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are pretty close, so we would have confidence the test and training data is nearly identical if we were making a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings\n",
    "[Raschkas's Lecture](https://github.com/rasbt/stat479-machine-learning-fs19/blob/master/11_eval4-algo/11-eval4-algo__slides.pdf)\n",
    "<br>[Full paper: Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning, Raschka](https://arxiv.org/abs/1811.12808)\n",
    "<br>[Evaluation: From Precision, Recall and F-Factor to ROC, Informedness, Markedness & Correlation](https://arxiv.org/abs/2010.16061)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
